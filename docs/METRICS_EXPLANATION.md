# Metrics Documentation

## üìö New Documentation Structure

This guide has been split into focused, manageable documents for easier navigation:

### **Start Here**: [Metrics Index](./metrics_index.md)
Central navigation hub with:
- Quick start guide for new users
- Shared concepts across all metrics
- Progressive learning path (Week 1 ‚Üí Week 2-4 ‚Üí Beyond)

### Specialized Guides

1. **[DORA Metrics Guide](./dora_metrics.md)** (~450 lines)
   - Deployment Frequency (deployments/week)
   - Lead Time for Changes (days from commit to deployed)
   - Change Failure Rate (% of failed deployments)
   - Mean Time to Recovery (hours from bug to fix)
   - Performance tiers: Elite/High/Medium/Low
   - Action plans for each metric

2. **[Flow Metrics Guide](./flow_metrics.md)** (~480 lines)
   - Flow Velocity (items/week by work type)
   - Flow Time (cycle time in days)
   - Flow Efficiency (active vs. waiting time %)
   - Flow Load / WIP (work in progress with dynamic thresholds)
   - Flow Distribution (Feature/Defect/Tech Debt/Risk balance)
   - Little's Law for WIP management

3. **[Dashboard Metrics Guide](./dashboard_metrics.md)** (~620 lines)
   - Project Health Score (0-100 composite score)
   - Completion Forecast (PERT-based estimation)
   - Current Velocity (throughput with Nov 12 bug fix)
   - Remaining Work (items/points tracking)
   - PERT Timeline (Optimistic/Most Likely/Pessimistic)
   - **Statistical Limitations** (velocity bug fix + known issues)
   - Practical usage scenarios

---

## Why the Split?

**Before**: 2,052-line monolithic file  
**After**: 4 focused documents (~400-500 lines each)

**Benefits**:
- ‚úÖ Faster navigation to specific metrics
- ‚úÖ Clearer focus per document
- ‚úÖ Shared concepts centralized (no duplication)
- ‚úÖ Statistical limitations prominently documented
- ‚úÖ Easier to maintain and update

---

## Quick Navigation

**New to metrics?** ‚Üí Start with [Metrics Index](./metrics_index.md) Quick Start guide

**Looking for process metrics?**
- DevOps/Deployment metrics ‚Üí [DORA Metrics Guide](./dora_metrics.md)
- Value stream/work type metrics ‚Üí [Flow Metrics Guide](./flow_metrics.md)

**Looking for project forecasting?**
- Dashboard overview ‚Üí [Dashboard Metrics Guide](./dashboard_metrics.md)
- Statistical limitations ‚Üí [Dashboard Guide - Statistical Limitations](./dashboard_metrics.md#statistical-limitations-and-known-issues)

---

## Original Content Preserved

All content from the original 2,052-line document has been preserved and reorganized. Nothing was removed - only restructured for better usability.

**If you're looking for a specific section**, use the table below:

| Original Section          | New Location                                                                               |
| ------------------------- | ------------------------------------------------------------------------------------------ |
| Quick Start               | [Metrics Index](./metrics_index.md)                                                        |
| DORA Metrics (all 4)      | [DORA Metrics Guide](./dora_metrics.md)                                                    |
| Flow Metrics (all 5)      | [Flow Metrics Guide](./flow_metrics.md)                                                    |
| Dashboard Metrics (all 5) | [Dashboard Metrics Guide](./dashboard_metrics.md)                                          |
| Dashboard Insights        | [Dashboard Metrics Guide](./dashboard_metrics.md#dashboard-insights-contextual-guidance)   |
| Statistical Limitations   | [Dashboard Metrics Guide](./dashboard_metrics.md#statistical-limitations-and-known-issues) |
| Time Units & Aggregations | [Metrics Index](./metrics_index.md#understanding-per-week-aggregations)                    |

---

## Legacy Content Below

The remainder of this file contains the original monolithic documentation for reference. **It is recommended to use the new focused guides above instead.**

---

## Overview (Legacy)

---

## Quick Start Guide

**New to DORA & Flow Metrics?** Start here:

### Week 1: Focus on These Core Metrics

1. **Deployment Frequency** (DORA) - How often you ship to production
   - **Why first**: Easy to measure, immediate visibility into delivery cadence
   - **Goal**: Aim for at least weekly deployments (Medium tier)
   - **Quick win**: Track current rate, set weekly deployment goal

2. **Flow Load (WIP)** (Flow) - How much work is in progress
   - **Why second**: Single biggest lever for improving speed
   - **Goal**: Keep WIP below calculated healthy threshold (green zone)
   - **Quick win**: Stop starting new work until WIP drops below warning threshold

3. **Flow Velocity** (Flow) - How much work you complete per week
   - **Why third**: Baseline for capacity planning and forecasting
   - **Goal**: Establish consistent, predictable throughput
   - **Quick win**: Track for 4 weeks to establish baseline velocity

### Week 2-4: Add Quality & Speed Metrics

4. **Change Failure Rate** (DORA) - Percentage of deployments that fail
   - **Goal**: Keep below 15% (Elite tier)
   - **Action**: If >30%, pause new features and fix deployment process

5. **Lead Time for Changes** (DORA) - Time from code ready to deployed
   - **Goal**: Under 1 week (Medium tier), under 1 day (High tier)
   - **Action**: If >1 week, investigate deployment bottlenecks

6. **Flow Time** (Flow) - Time from start to completion
   - **Goal**: Consistent, predictable cycle time (varies by team)
   - **Action**: If increasing, check WIP and identify blockers

### Beyond Week 4: Optimize

7. **MTTR** (DORA) - Time to recover from production incidents
8. **Flow Efficiency** (Flow) - Active work time vs. waiting time
9. **Flow Distribution** (Flow) - Balance of feature/defect/tech debt/risk work

**Progressive Approach**: Don't try to optimize all metrics at once. Focus on 2-3 at a time, establish baselines, then expand.

---

## Metric Relationships: How They Connect

Understanding how metrics interact helps you make better decisions:

### The WIP-Speed Connection (Most Important!)

```
High WIP ‚Üí Slower Flow Time ‚Üí Slower Lead Time ‚Üí Lower Deployment Frequency
  ‚Üì
Context Switching + Blocked Items + Lost Focus
```

**Why it matters**: Flow Load (WIP) is your control lever. Reduce WIP ‚Üí Everything else improves.

**Example**:
- Team has 45 items in WIP (Critical) with 8-day Flow Time
- Reduce WIP to 15 items (Warning) ‚Üí Flow Time drops to 5 days
- Faster Flow Time ‚Üí Faster Lead Time ‚Üí More frequent deployments

### The Quality-Speed Balance

```
Fast Deployment + Low CFR = Elite Performance ‚úÖ
Fast Deployment + High CFR = Technical Debt Accumulation ‚ùå
Slow Deployment + Low CFR = Risk Aversion (missing opportunities) ‚ö†Ô∏è
```

**Optimal Zone**: Deploy frequently (‚â•1/week) while maintaining CFR <15%.

**Warning Signs**:
- CFR >30% + Increasing Deployment Frequency = Quality problems, slow down!
- CFR <5% + Low Deployment Frequency = Overengineering, ship more!

### The Efficiency Paradox

```
Flow Efficiency = Active Time / Total Time

100% Efficiency = No buffer time = System overload ‚ùå
  0% Efficiency = All waiting = Process broken ‚ùå
25-40% Efficiency = Healthy balance = Sustainable pace ‚úÖ
```

**Counterintuitive**: Lower efficiency (25-40%) is BETTER than higher efficiency (>60%) because:
- Teams need planning time, code review time, testing time
- Buffer capacity prevents cascading delays when issues arise
- Sustainable pace prevents burnout

**Action**: If efficiency >60%, you're likely overloaded (check WIP). If <20%, investigate bottlenecks.

### The Distribution-Quality Connection

```
Feature Work >70% + Defect Work <10% = Future quality problems ‚ö†Ô∏è
Feature Work <40% + Defect Work >40% = Already in quality crisis ‚ùå
Balanced: Features 40-60%, Defects 20-40%, Tech Debt 10-20% ‚úÖ
```

**Leading Indicator**: Flow Distribution predicts future CFR and MTTR.

**Warning Signs**:
- Skewing toward features (>70%) ‚Üí Expect CFR to rise in 4-8 weeks
- Skewing toward defects (>40%) ‚Üí Quality crisis already in progress

---

## Common Pitfalls & Anti-Patterns

Avoid these mistakes when using metrics:

### 1. Gaming the Metrics ‚ùå

**Anti-Pattern**: Optimizing one metric at the expense of others.

**Examples**:
- Deploying tiny changes constantly to boost Deployment Frequency (but Lead Time stays high)
- Classifying production bugs as non-PROD to lower MTTR
- Splitting work into tiny tasks to boost Flow Velocity (but nothing valuable ships)

**Solution**: Always view metrics as a **balanced scorecard**. Elite teams excel across ALL metrics, not just one.

### 2. Comparing Across Teams ‚ùå

**Anti-Pattern**: Using metrics to rank teams or individuals.

**Why it fails**:
- Different team contexts (greenfield vs. legacy, microservices vs. monolith)
- Different business constraints (compliance, security requirements)
- Creates perverse incentives (gaming, sandbagging, hiding problems)

**Solution**: Each team competes against their **own historical baseline**, not other teams.

### 3. Setting Arbitrary Targets ‚ùå

**Anti-Pattern**: "All teams must achieve Elite tier in all metrics by Q2."

**Why it fails**:
- Ignores team context and constraints
- Creates pressure to game metrics
- Elite tier may not be necessary or cost-effective for all contexts

**Solution**: Set goals based on **business impact** ("reduce customer-impacting incidents by 50%") and let metrics guide improvement.

### 4. Focusing on Lagging Indicators Only ‚ùå

**Anti-Pattern**: Only tracking DORA metrics (outcomes) without Flow metrics (process).

**Why it fails**:
- DORA metrics tell you WHAT happened, not WHY
- Can't improve what you don't measure at the process level

**Solution**: Use Flow metrics to understand and improve the process. DORA metrics validate that improvements worked.

**Example**:
- DORA: "Lead Time increased 30% this month" ‚Üê What happened
- Flow: "WIP increased from 15 to 35 items" ‚Üê Why it happened
- Action: "Implement WIP limits to restore Lead Time" ‚Üê How to fix

### 5. Analysis Paralysis ‚ùå

**Anti-Pattern**: Calculating every metric, creating dashboards, but never taking action.

**Why it fails**:
- Metrics are a means to improvement, not the end goal
- Data without action is waste

**Solution**: For each metric review, identify 1-2 **actionable experiments**:
- "Try WIP limit of 12 items for 2 weeks, measure impact on Flow Time"
- "Add automated deployment tests, measure impact on CFR"

### 6. Ignoring Outliers ‚ùå

**Anti-Pattern**: Only looking at median/average values.

**Why it fails**:
- P95 values reveal worst-case scenarios that frustrate customers
- Long tail issues indicate systemic problems

**Solution**: Always check P95 alongside median. If P95 is 3x+ higher than median, investigate outliers.

### 7. Short-Term Thinking ‚ùå

**Anti-Pattern**: Reacting to week-to-week fluctuations.

**Why it fails**:
- Metrics naturally vary week to week
- Overreacting creates thrashing

**Solution**: Look for **trends over 4+ weeks**. One bad week is noise. Four bad weeks is a signal.

---

## Reading the Metric Cards (Visual Guide)

Each metric card provides multiple visual signals to help you quickly assess performance:

### Performance Tier Badge
- **Elite** (Green): Top-tier performance per DORA/Flow standards
- **High** (Blue): Above average performance
- **Medium** (Yellow): Average performance, room for improvement
- **Low** (Red): Below average, needs attention
- **Special for Flow Load (WIP)**: Shows health status instead
  - **Healthy** (Green): <10 items in progress
  - **Warning** (Yellow): 10-19 items
  - **High** (Orange): 20-29 items
  - **Critical** (Red): ‚â•30 items

### Trend Indicators (Below Main Value)
Shows how the current period compares to historical average:

- **‚Üí (Right Arrow, Gray)**: Stable - exactly 0.0% change from previous average
  - Example: "‚Üí 0.0% vs prev avg"
  - Indicates consistent, predictable performance
  
- **‚Üë (Up Arrow)**: Increase of ‚â•5% from previous average
  - **Green**: Good improvement (Deployment Frequency)
  - **Red**: Concerning increase (Lead Time, MTTR, CFR, WIP)
  - Example: "‚Üë 12.5% vs prev avg"
  
- **‚Üì (Down Arrow)**: Decrease of ‚â•5% from previous average
  - **Green**: Good improvement (Lead Time, MTTR, CFR, WIP)
  - **Red**: Concerning decrease (Deployment Frequency)
  - Example: "‚Üì 8.3% vs prev avg"
  
- **‚àí (Minus, Gray)**: Small change (<5%) or insufficient historical data
  - Example: "‚àí 2.1% vs prev avg" or "‚àí No trend data yet"
  - Indicates stability within normal variance

**How to Read Trends**:
- Compare current period's median/average to previous periods' average
- Helps identify improving/degrading trends before they become critical
- Gray indicators (‚Üí and ‚àí) mean "stable" - neither good nor bad

### Secondary Metrics (Below Trend)
Provides additional context for deeper analysis:

**For Deployment Frequency & Change Failure Rate**:
- Shows unique releases (vs. deployment tasks)
- Example: "üì¶ 1.9 releases/week"
- Helps distinguish release cadence from deployment process complexity

**For Lead Time & MTTR**:
- **P95**: 95th percentile - worst-case scenarios (5% of issues exceed this)
- **Mean**: Arithmetic average - capacity planning baseline
- Example: "üìä P95: 12.5d ‚Ä¢ Avg: 8.2d"
- Useful for SLA setting and identifying outliers

### Inline Sparkline
- Mini-chart below secondary metrics shows weekly trend
- Quick visual pattern recognition (upward/downward/stable)
- Colored by performance tier

### Card Height Consistency
All cards maintain uniform height regardless of data availability:
- Cards with trends show trend indicators
- Cards without sufficient data show "No trend data yet" placeholder
- Ensures clean, professional grid layout

### Detail Charts (Click to Expand)
Click any DORA metric card to view detailed weekly breakdown:

**DORA Metrics - Performance Tier Zones**:
- Background zones show Elite/High/Medium/Low thresholds
- Your weekly data points plotted against industry standards
- Helps visualize where you are vs. where you want to be
- Color-coded zones:
  - **Green**: Elite performance (top 10% of teams)
  - **Blue**: High performance (top 25%)
  - **Yellow**: Medium performance (top 50%)
  - **Red**: Low performance (needs improvement)
- Example: Lead Time chart shows zones at <1h (Elite), 1d-1w (High), 1w-1mo (Medium), >1mo (Low)

**Flow Metrics - Trend Analysis**:
- Standard trend charts without zones (no universal Flow standards exist)
- Focus on your team's historical patterns and trends
- Useful for tracking improvements over time

**Work Distribution Breakdown**:
- Stacked area chart shows Feature/Defect/Tech Debt/Risk evolution over time
- **Mobile-First Design**: Target zones clearly visible with 15-18% opacity for immediate value
- **Enhanced Hover**: Shows current percentage, item count, and target range for each work type
- **Progressive Disclosure**: Target ranges shown in subtitle and hover details
- **Responsive Layout**: Horizontal legend below chart (mobile-friendly) vs. vertical sidebar (desktop)
- **Visual Zones**: Color-coded target ranges make it easy to assess healthy work balance
  - **Green Zone (40-60%)**: Feature work - new capabilities and business value
  - **Red Zone (20-40%)**: Defect work - quality maintenance and bug fixes  
  - **Orange Zone (10-20%)**: Tech Debt - infrastructure and sustainability
  - **Yellow Zone (0-10%)**: Risk work - security, compliance, prevention
- Current week summary with recommended ranges and indicators
- Helps maintain healthy work balance and identify portfolio imbalances

## Time Periods & Aggregation Strategy

### Weekly Bucketing (ISO Weeks)
- **Week Definition**: Monday to Sunday (ISO 8601 standard)
- **Week Labels**: Format `YYYY-WW` (e.g., "2025-45")
- **Data Points Slider**: Controls how many historical weeks to display (default: 12 weeks)

### When Metrics Are Calculated
- **Manual Trigger**: User clicks "Calculate Metrics" button in Settings
- **Process**: Calculates metrics for last N weeks and saves to `metrics_snapshots.json`
- **Duration**: ~2 minutes for 12 weeks of data
- **Display**: Instant (reads from cache)

---

## DORA Metrics (DevOps Research & Assessment)

### 1. Deployment Frequency

**What it measures**: How often deployments happen to production

**Important Distinction**: 
- **Deployment** = An operational task (deployment activity/process)
- **Release** = A unique fixVersion (the actual code release)
- **Example**: 2 operational tasks with fixVersion `R_20251104_example` = **2 deployments, 1 release**

**Calculation Method**:
- **Per Week (Deployments)**: COUNT of operational tasks with `fixVersion.releaseDate` in that week
- **Per Week (Releases)**: COUNT of UNIQUE fixVersion names in that week
- **Absolute Count**: Each week shows actual numbers
- **Filtering**:
  - Only tasks where `status IN flow_end_statuses` (Done, Closed, etc.)
  - Only tasks with `fixVersion.releaseDate <= today` (excludes future deployments)
  - Uses **earliest releaseDate** if multiple fixVersions exist

**Display Value** (Card):
- **Primary Unit**: `deployments/week (avg Nw)`
- **Primary Calculation**: `AVERAGE(weekly_deployment_counts)` across N weeks
- **Secondary Display**: `releases/week` (shown below primary value)
- **Secondary Calculation**: `AVERAGE(weekly_release_counts)` across N weeks
- **Trend Indicator**: Shows % change vs. previous average (‚Üë green = good, ‚Üì red = concerning)
- **Performance Badge**: Elite (‚â•30/mo) ‚Üí High (‚â•7/mo) ‚Üí Medium (‚â•1/mo) ‚Üí Low (<1/mo)
- **Example**: 12 weeks with deployment counts `[2, 3, 4, 2, 5, 3, 2, 4, 3, 2, 3, 4]` and release counts `[1, 2, 3, 1, 3, 2, 1, 2, 2, 1, 2, 3]`
  - Average deployments = 3.1 deployments/week
  - Average releases = 1.9 releases/week
  - Card shows: "3.1 deployments/week" with "üì¶ 1.9 releases/week" below

**Scatter Chart** (Weekly Data Points):
- **X-axis**: Week labels (2025-45, 2025-46, etc.)
- **Y-axis**: Absolute deployment count for that week
- **Values**: Each point = actual deployments in that week (NOT cumulative, NOT average)
- **Background Zones**: Elite/High/Medium/Low performance tiers per DORA standards
- **Note**: Currently shows deployments only. Could be enhanced to show both metrics or releases separately.

**Trend Line**:
- Shows progression of weekly deployment frequency over time

**Why Track Both?**:
- Multiple operational tasks (deployments) for one release indicates:
  - Complex deployment process (multiple stages: staging ‚Üí production)
  - Deployment rollback and retry scenarios
  - Multiple environment deployments tracked separately
- Helps identify deployment efficiency vs. release cadence

**Code Location**: `data/dora_calculator.py::calculate_deployment_frequency_v2()` and `aggregate_deployment_frequency_weekly()`

---

### 2. Lead Time for Changes

**What it measures**: Time from code deployment-ready to deployed in production

**Calculation Method**:
- **Start Time**: First "In Deployment" status timestamp (from changelog)
  - **Fallback**: First "Done" status if never reached "In Deployment"
- **End Time**: Matching operational task's `fixVersion.releaseDate`
  - Matching logic: Development issue fixVersion ID matches operational task fixVersion ID
  - **Fallback**: Issue's `resolutiondate` if no operational task found
- **Time Unit**: **HOURS**
- **Per Week**: All issues where deployment happened in that week
- **Aggregation**: **MEDIAN** of all lead times in that week (not mean - robust to outliers)

**Display Value** (Card):
- **Primary Unit**: `days (16w median avg)`
- **Primary Calculation**: 
  1. Calculate median hours for each week
  2. Convert to days: `median_hours / 24`
  3. Average the weekly medians: `AVERAGE(weekly_median_days)` across 16 weeks
- **Secondary Display**: P95 and Mean averages (shown below primary value)
  - **P95**: 95th percentile - only 5% of issues take longer than this
  - **Mean**: Arithmetic average - useful for capacity planning
  - Both averaged across the same 16-week period
- **Trend Indicator**: Shows % change vs. previous average (‚Üì green = improvement, ‚Üë red = regression)
- **Performance Badge**: Elite (<1h) ‚Üí High (<1d) ‚Üí Medium (<1w) ‚Üí Low (>1w)
- **Example**: 
  - Week 1: Issues with lead times [24h, 48h, 72h] ‚Üí Median = 48h (2d), Mean = 48h (2d), P95 = 67.2h (2.8d)
  - Week 2: Issues with lead times [36h, 60h] ‚Üí Median = 48h (2d), Mean = 48h (2d), P95 = 57.6h (2.4d)
  - Averages: Median = 2d, Mean = 2d, P95 = 2.6d
- **Card Display**:
  - Primary: "2 days (16w median avg)"
  - Trend: "‚Üì 5.2% vs prev avg" (green = improving)
  - Secondary: "üìä P95: 2.6d ‚Ä¢ Avg: 2d"

**Scatter Chart** (Weekly Data Points):
- **X-axis**: Week labels
- **Y-axis**: Median lead time in **days** for that week
- **Values**: Each point = median of all lead times in that week
- **Background Zones**: Elite/High/Medium/Low performance tiers per DORA standards

**Why Median, Not Mean?**
- Outliers (e.g., 1 issue taking 200 hours) don't skew the metric
- More representative of "typical" lead time

**Why Show P95 and Mean?**
- **P95**: Helps identify outliers and set realistic SLAs (95% of changes complete within this time)
- **Mean**: Useful for capacity planning and resource allocation
- Together they provide a complete picture of lead time distribution

**Code Location**: `data/dora_calculator.py::calculate_lead_time_for_changes_v2()` and `aggregate_lead_time_weekly()`

---

### 3. Change Failure Rate (CFR)

**What it measures**: Percentage of deployments that fail and require remediation

**Calculation Method**:
- **Per Week**: 
  - Total deployments = COUNT(operational tasks with releaseDate in week)
  - Failed deployments = COUNT(operational tasks with `customfield_10001 == "Yes"`)
  - CFR = `(failed / total) √ó 100`
- **Critical**: ONLY `"Yes"` (case-insensitive) indicates failure
  - `"No"`, `"None"`, `null`, empty = success
- **Filtering**: Same as Deployment Frequency (completion statuses, releaseDate <= today)

**Display Value** (Card):
- **Unit**: `% (agg Nw)` (agg = aggregated)
- **Calculation**: **AGGREGATE** across N weeks, NOT average of weekly rates
  - `total_deployments = SUM(all weekly deployment counts)`
  - `total_failures = SUM(all weekly failure counts)`
  - `CFR = (total_failures / total_deployments) √ó 100`
- **Trend Indicator**: Shows % change vs. previous average (‚Üì green = improvement, ‚Üë red = regression)
- **Performance Badge**: Elite (0-15%) ‚Üí High (16-30%) ‚Üí Medium (31-45%) ‚Üí Low (>45%)
- **Special Case**: Shows "‚Üí 0.0% vs prev avg" when stable at 0% (no failures)
- **Example**:
  - Week 1: 10 deployments, 1 failure ‚Üí 10%
  - Week 2: 20 deployments, 0 failures ‚Üí 0%
  - Aggregate: (1 / 30) √ó 100 = 3.3% ‚úì (NOT (10% + 0%) / 2 = 5% ‚úó)
  - Card shows: "3.3%" with "‚Üí 0.0% vs prev avg" if stable

**Scatter Chart** (Weekly Data Points):
- **X-axis**: Week labels
- **Y-axis**: Failure rate **percentage** for that week
- **Values**: Each point = `(failures / deployments) √ó 100` for that week
- **Background Zones**: Elite/High/Medium/Low performance tiers per DORA standards

**Why Aggregate, Not Average?**
- Prevents weeks with few deployments from skewing the metric
- More accurate representation of overall failure rate

**Code Location**: `data/dora_calculator.py::calculate_change_failure_rate_v2()` and `aggregate_change_failure_rate_weekly()`

---

### 4. Mean Time to Recovery (MTTR)

**What it measures**: Time from production bug creation to fix deployed

**Calculation Method**:
- **Bug Filtering**: 
  - Issue type = "Bug"
  - `customfield_10002 == "PROD"` (exact match, case-sensitive)
- **Start Time**: `bug.fields.created` (when bug was reported)
- **End Time**: 
  - **Preferred**: Matching operational task's `fixVersion.releaseDate`
  - **Fallback**: Bug's `resolutiondate` if no operational task found
- **Time Unit**: **HOURS**
- **Per Week**: All bugs where fix was deployed in that week
- **Aggregation**: **MEDIAN** of all recovery times in that week

**Display Value** (Card):
- **Primary Unit**: `hours (16w median avg)`
- **Primary Calculation**:
  1. Calculate median hours for each week
  2. Average the weekly medians: `AVERAGE(weekly_median_hours)` across 16 weeks
- **Secondary Display**: P95 and Mean averages (shown below primary value)
  - **P95**: 95th percentile - only 5% of bugs take longer to fix
  - **Mean**: Arithmetic average - useful for capacity planning
  - Both averaged across the same 16-week period
- **Trend Indicator**: Shows % change vs. previous average (‚Üì green = improvement, ‚Üë red = regression)
- **Performance Badge**: Elite (<1h) ‚Üí High (<1d) ‚Üí Medium (<1w) ‚Üí Low (>1w)
- **Example**:
  - Week 1: Bugs with recovery times [12h, 24h, 48h] ‚Üí Median = 24h, Mean = 28h, P95 = 44.4h
  - Week 2: Bugs with recovery times [18h, 36h] ‚Üí Median = 27h, Mean = 27h, P95 = 34.2h
  - Averages: Median = 25.5h, Mean = 27.5h, P95 = 39.3h
- **Card Display**:
  - Primary: "25.5 hours (16w median avg)"
  - Trend: "‚Üì 3.8% vs prev avg" (green = improving)
  - Secondary: "üìä P95: 39.3h ‚Ä¢ Avg: 27.5h"

**Scatter Chart** (Weekly Data Points):
- **X-axis**: Week labels
- **Y-axis**: Median MTTR in **hours** for that week
- **Values**: Each point = median of all recovery times in that week
- **Background Zones**: Elite/High/Medium/Low performance tiers per DORA standards

**Why Show P95 and Mean?**
- **P95**: Helps set incident response SLAs (95% of bugs fixed within this time)
- **Mean**: Useful for team capacity planning and on-call rotations
- Together they help identify both typical recovery time and worst-case scenarios

**Code Location**: `data/dora_calculator.py::calculate_mttr_v2()` and `aggregate_mttr_weekly()`

---

## Flow Metrics (Flow Framework - Mik Kersten)

### 1. Flow Velocity

**What it measures**: Number of work items completed per time period

**Calculation Method**:
- **Per Week**: COUNT of issues with `status IN flow_end_statuses` AND `resolutiondate` in that week
- **Classification**: Two-tier system using `flow_type_mappings` from `app_settings.json`:
  - **Primary**: Issue type (Bug ‚Üí Defect, Story/Task ‚Üí Feature by default)
  - **Secondary**: Effort category (overrides primary for Task/Story only)
    - "Technical debt" ‚Üí Technical Debt
    - "Security", "GDPR", etc. ‚Üí Risk
    - Otherwise ‚Üí Feature
- **Absolute Count**: Each week shows actual number of completed items

**Display Value** (Card):
- **Unit**: `items/week` (current week only, NOT average)
- **Calculation**: Total items completed in the **most recent week**
- **Example**: Week 2025-45 completed 25 items ‚Üí Display: "25 items/week"

**Breakdown**:
- Feature: X items
- Defect: Y items
- Tech Debt: Z items
- Risk: W items

**Scatter Chart** (Weekly Data Points):
- **X-axis**: Week labels
- **Y-axis**: Absolute count of completed items for that week
- **Values**: Each point = actual completions in that week

**Stacked Area Chart** (Work Distribution Over Time):
- Shows how work types (Feature/Defect/Tech Debt/Risk) evolved week by week
- Hover shows percentages for each work type

**Code Location**: `data/flow_calculator.py::calculate_flow_velocity_v2()` and `aggregate_flow_velocity_weekly()`

---

### 2. Flow Time

**What it measures**: Average time from work start to completion

**Calculation Method**:
- **Start Time**: First transition to ANY `wip_statuses` (from changelog)
  - Example wip_statuses: ["Selected", "In Progress", "In Review", "Testing"]
- **End Time**: `resolutiondate` field
- **Time Unit**: **DAYS** (calculated as hours / 24)
- **Per Week**: All issues that completed (resolutiondate) in that week
- **Aggregation**: **MEDIAN** of all flow times in that week

**Display Value** (Card):
- **Unit**: `days (avg Nw)`
- **Calculation**:
  1. Calculate median days for each week
  2. Average the weekly medians: `AVERAGE(weekly_median_days)` across N weeks
- **Example**:
  - Week 1: Issues with flow times [2d, 4d, 6d] ‚Üí Median = 4d
  - Week 2: Issues with flow times [3d, 5d] ‚Üí Median = 4d
  - Average = (4 + 4) / 2 = 4 days

**Scatter Chart** (Weekly Data Points):
- **X-axis**: Week labels
- **Y-axis**: Median flow time in **days** for that week
- **Values**: Each point = median of all flow times in that week

**Code Location**: `data/flow_calculator.py::calculate_flow_time_v2()` and `aggregate_flow_time_weekly()`

---

### 3. Flow Efficiency

**What it measures**: Percentage of time spent actively working vs. waiting

**Calculation Method**:
- **Active Time**: Total time in `active_statuses` (from changelog)
  - Example: ["In Progress", "In Review", "Testing"]
- **Total WIP Time**: Total time in `wip_statuses` (from changelog)
  - Example: ["Selected", "In Progress", "In Review", "Testing", "Ready for Testing"]
- **Efficiency**: `(active_time / wip_time) √ó 100`
- **Per Week**: All issues that completed in that week
- **Aggregation**: 
  1. Sum all active hours across all issues in week
  2. Sum all WIP hours across all issues in week
  3. `(total_active / total_wip) √ó 100`

**Display Value** (Card):
- **Unit**: `%` (current week only, NOT average)
- **Calculation**: Efficiency percentage for the **most recent week**
- **Example**: Week 2025-45 had 420h active, 720h WIP ‚Üí (420/720)√ó100 = 58.3%

**Scatter Chart** (Weekly Data Points):
- **X-axis**: Week labels
- **Y-axis**: Efficiency percentage for that week
- **Values**: Each point = `(total_active / total_wip) √ó 100` for that week

**Healthy Range**: 25-40% (higher = less waiting, but 100% = no buffer/planning time)

**Code Location**: `data/flow_calculator.py::calculate_flow_efficiency_v2()` and `aggregate_flow_efficiency_weekly()`

---

### 4. Flow Load (WIP)

**What it measures**: Current work-in-progress count

**Calculation Method**:
- **Per Week**: COUNT of issues where `status IN wip_statuses` at end of that week
- **Snapshot**: Point-in-time measurement (not cumulative)
- **wip_statuses** from config (e.g., ["Selected", "In Progress", "Testing", "Ready for Testing", "In Deployment"])

**Historical Reconstruction**:
For historical weeks, the system reconstructs what WIP was at that specific point in time:

1. **Load All Issues** from cache (development project only, excludes operational projects)
2. **Determine Week End Time**:
   - Historical weeks: Sunday 23:59:59 of that week
   - Current week: NOW (running total)
3. **Reconstruct Status for Each Issue**:
   - **Current Week**: Use current status directly (faster, includes issues without changelog)
   - **Historical Weeks**: Replay changelog history
     - Find LAST status change BEFORE week end
     - If issue was in WIP status at that time ‚Üí count it
     - If no changelog exists ‚Üí use current status (assumes issue stayed in same status)
4. **Count Issues** where reconstructed status is in `wip_statuses` AND resolution is empty

**Example Timeline**:
```
Issue DEV-123 (Task):
‚îú‚îÄ Created: Week 10 (status: "To Do")
‚îú‚îÄ Week 15: Moved to "Selected" ‚Üê WIP starts, counted in W15+
‚îú‚îÄ Week 20: Moved to "In Progress" ‚Üê Still in WIP
‚îú‚îÄ Week 25: Moved to "Done" ‚Üê WIP ends, not counted from W25+

Week-by-week WIP count for this issue:
W10-W14: NOT counted (status = "To Do", not in wip_statuses)
W15-W24: COUNTED (status in wip_statuses)
W25+: NOT counted (status = "Done", in flow_end_statuses)
```

**Issue Filtering**:
- ‚úÖ **Development Project** issues only (e.g., project key "DEV")
- ‚úÖ **Issue Types**: Task, Story, Bug (configurable)
- ‚ùå **Operational/DevOps** projects excluded (used only for DORA deployment tracking)
- ‚ùå **Operational Task** issue type excluded

**Dynamic Health Thresholds (Little's Law)**:

The health badge thresholds are **calculated from your team's historical data** using Little's Law from queueing theory:

**Formula**: `Optimal WIP = Throughput √ó Cycle Time`
- **Throughput**: Weekly velocity (items completed per week)
- **Cycle Time**: Flow time in days (time from start to completion)

**Calculation Process**:
1. For each historical week with data:
   - `optimal_wip = velocity √ó (flow_time_days / 7)`
   - Example: 18 items/week √ó (6 days / 7) = 15.4 optimal WIP
2. Collect all optimal WIP values across historical period
3. Calculate percentiles from distribution:
   - **P25** (25th percentile): 25% of weeks had WIP below this
   - **P50** (median): 50% of weeks had WIP below this
   - **P75** (75th percentile): 75% of weeks had WIP below this
   - **P90** (90th percentile): Only 10% of weeks exceeded this
4. Apply 20% stability buffers to create zones:
   - **Healthy** (Green): < P25 √ó 1.2 (below 25th percentile with buffer)
   - **Warning** (Yellow): < P50 √ó 1.2 (below median with buffer)
   - **High** (Orange): < P75 √ó 1.2 (below 75th percentile with buffer)
   - **Critical** (Red): ‚â• P90 (at or above 90th percentile - danger zone)

**Example Calculation**:
```
Historical data (25 weeks):
- Optimal WIP values: [8.2, 10.5, 12.1, 9.8, 15.3, 11.7, ...]
- P25 = 8.3, P50 = 14.8, P75 = 23.6, P90 = 41.9

Thresholds with buffers:
- Healthy: 8.3 √ó 1.2 = 10.0 items
- Warning: 14.8 √ó 1.2 = 17.7 items
- High: 23.6 √ó 1.2 = 28.3 items
- Critical: 41.9 (no buffer for danger zone)
```

**Why This Matters**:
- ‚úÖ **Team-Specific**: Adapts to YOUR actual capacity and cycle time
- ‚úÖ **Data-Driven**: Based on historical performance, not arbitrary numbers
- ‚úÖ **Context-Aware**: Accounts for team size and work complexity
- ‚úÖ **Predictive**: Higher WIP correlates with slower cycle time (validated by queueing theory)

**Fallback Thresholds**:
If insufficient historical data or NumPy unavailable:
- Healthy: <10 items
- Warning: <20 items
- High: <30 items
- Critical: ‚â•40 items

(These defaults are reasonable for teams of 5-10 people)

**Display Value** (Card):
- **Unit**: `items (current WIP)` - shows most recent week
- **Calculation**: WIP count for the **most recent week**
- **Health Badge**: Dynamic thresholds from Little's Law (or fallback if needed)
- **Trend Indicator**: Shows % change vs. previous average (‚Üì green = improvement, ‚Üë red = growing WIP)
- **Tooltip**: Explains Little's Law methodology and threshold calculation
- **Example**: Current week has 59 items with thresholds Healthy<10.0, Warning<17.7, High<28.3, Critical‚â•41.9
  - Display: "59 items" with "Critical" badge (red)
  - Indicates: Team has too much WIP, focus on finishing before starting new work

**Scatter Chart with Threshold Lines** (Weekly Data Points):
- **X-axis**: Week labels
- **Y-axis**: WIP count for that week
- **Main Line**: Blue line showing WIP trend over time
- **Threshold Lines** (Hoverable):
  - üü¢ **Green dotted line**: Healthy threshold (hover shows exact value)
  - üü° **Yellow dotted line**: Warning threshold (hover shows exact value)
  - üü† **Orange dotted line**: High threshold (hover shows exact value)
  - üî¥ **Red dashed line**: Critical threshold (hover shows exact value)
- **Values**: Each point = actual WIP at end of that week
- **Hover**: Shows exact threshold values and WIP count for each week

**Interpreting the Chart**:
```
WIP
‚îÇ
60‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üî¥ Critical (‚â•41.9)
  ‚îÇ              ‚óè
50‚îÇ             ‚ï± ‚ï≤
  ‚îÇ            ‚ï±   ‚óè
40‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üü† High (<28.3)
  ‚îÇ                  ‚ï≤
30‚îÇ                   ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè
  ‚îÇ                          ‚ï≤
20‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üü° Warning (<17.7)
  ‚îÇ                             ‚ï≤
10‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ üü¢ Healthy (<10.0)
  ‚îÇ
 0‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
   W20  W25  W30  W35  W40  W45

Analysis:
- Weeks 20-30: WIP climbing from healthy to critical (red flag!)
- Week 35: Peak at 52 items - team severely overloaded
- Weeks 35-45: Declining trend (good!) but still in warning zone
- Goal: Get below green line (10 items) for optimal flow
```

**Why WIP Matters**:
- **Context Switching**: High WIP = team juggling too many items = slower delivery
- **Focus**: Low WIP = team finishes work faster with fewer interruptions
- **Predictability**: Stable WIP = predictable cycle time = reliable planning
- **Team Health**: Critical WIP (red) = burnout risk, quality issues, missed deadlines

**Recommended Actions by Zone**:
- üü¢ **Healthy**: Maintain current WIP limit, team is operating optimally
- üü° **Warning**: Review WIP limit, start finishing before starting new work
- üü† **High**: Implement strict WIP limits, focus on completing in-flight work
- üî¥ **Critical**: STOP starting new work, swarm to finish existing items, investigate bottlenecks

**Code Location**: `data/flow_calculator.py::calculate_flow_load_v2()`, `calculate_wip_thresholds_from_history()`, and `aggregate_flow_load_weekly()`

---

### 5. Flow Distribution

**What it measures**: Percentage breakdown of work by type

**Calculation Method**:
- **Per Week**: For all completed items in that week:
  - Count by Flow type (Feature, Defect, Tech Debt, Risk)
  - Calculate percentages: `(count / total) √ó 100`
- **Recommended Ranges** (Flow Framework standards):
  - **Feature: 40-60%** - New business value and capabilities (growth)
  - **Defect: 20-40%** - Quality maintenance and bug fixes (stability)  
  - **Tech Debt: 10-20%** - Infrastructure improvements and sustainability
  - **Risk: 0-10%** - Security, compliance, and operational risk mitigation

**Display Value** (Card shows Stacked Area Chart):
- **X-axis**: Week labels
- **Y-axis**: Count of items by type (stacked)
- **Hover**: Shows percentage for each type
- **Current Week Summary**: Shows counts and percentages with range indicators

**Actionable Guidance by Distribution Pattern**:

### Healthy Balance (‚úÖ Sustainable Growth)
```
Features: 40-60%  ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
Defects:  20-30%  ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
Tech Debt: 10-20% ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
Risk:      5-10%  ‚îÇ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
```
**What it means**: Team is delivering value while maintaining quality and addressing technical health.
**Action**: Maintain current balance. Monitor for shifts.

### Feature-Heavy (‚ö†Ô∏è Quality Debt Accumulating)
```
Features: >70%    ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚îÇ
Defects:  <15%    ‚îÇ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
Tech Debt: <5%    ‚îÇ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
Risk:      <5%    ‚îÇ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
```
**What it means**: Short-term velocity at expense of quality. Expect CFR and MTTR to increase in 4-8 weeks.
**Warning Signs**: 
- Code getting harder to change
- Bug count increasing
- Team velocity slowing despite more features
**Action**: 
- Allocate 20% capacity to Tech Debt immediately
- Increase automated test coverage
- Schedule architecture review

### Defect Crisis (‚ùå Quality Problems Manifested)
```
Features: <30%    ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
Defects:  >50%    ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚îÇ
Tech Debt: <10%   ‚îÇ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
Risk:      <5%    ‚îÇ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
```
**What it means**: Team overwhelmed with bugs, little progress on new value.
**Warning Signs**:
- Customer complaints increasing
- Team morale declining
- Velocity collapsing
**Action**: 
- **STOP starting new features** until defect rate drops below 40%
- Implement bug rotation (dedicated person per day/week)
- Root cause analysis: Why are so many bugs being created?
- Consider quality freeze: Fix top 10 most painful bugs before anything else

### Tech Debt Cleanup Mode (‚ö†Ô∏è Investment Period)
```
Features: 20-30%  ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
Defects:  15-25%  ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
Tech Debt: 40-50% ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
Risk:      5-10%  ‚îÇ‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
```
**What it means**: Deliberate investment in technical health (refactoring, upgrades, migrations).
**When appropriate**:
- After major release
- Before starting new major initiative
- When velocity has slowed due to technical issues
**Action**: 
- Set time-box (e.g., "Tech Debt sprint for 2 weeks")
- Define clear outcomes (e.g., "Upgrade to Node 20", "Eliminate deprecated API calls")
- Return to balanced distribution after time-box ends

### Risk-Heavy (‚ö†Ô∏è Compliance/Security Focus)
```
Features: 20-30%  ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
Defects:  20-30%  ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
Tech Debt: 10-15% ‚îÇ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
Risk:      30-40% ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚îÇ
```
**What it means**: Heavy security/compliance work (audits, certifications, vulnerability remediation).
**When appropriate**:
- Pre-audit preparation
- Post-security incident response
- Regulatory compliance deadlines
**Action**: 
- Set time-box for risk work
- Communicate impact to stakeholders (reduced feature velocity)
- Return to balanced distribution after compliance achieved

**Pro Tip**: Use 4-week rolling average to smooth weekly fluctuations. React to trends, not individual weeks.

**Code Location**: `data/flow_calculator.py::aggregate_flow_distribution_weekly()`

---

## Project Dashboard Metrics

The **Project Dashboard** is the primary landing view, providing at-a-glance visibility into project health, completion forecast, velocity, and remaining work. Unlike DORA/Flow metrics (which measure process performance), Dashboard metrics focus on **project outcomes** and **delivery forecasting**.

### Dashboard Overview

**Purpose**: Single-page view of project status for stakeholders, product owners, and teams.

**Key Metrics**:
1. **Project Health Score** (0-100): Composite indicator of project status
2. **Completion Forecast**: PERT-based estimate of completion date
3. **Current Velocity**: Team throughput (items/week)
4. **Remaining Work**: Items and story points left to complete
5. **PERT Timeline**: Optimistic/Most Likely/Pessimistic completion dates

**Update Frequency**: Real-time (recalculated on every data refresh)

---

### 1. Project Health Score

**What it measures**: Overall project health as a single 0-100 score

**Calculation Method (Formula v2.1)**:
- **Four-component continuous formula** providing smooth, gradual health changes:

  **Total Health = Progress (30 pts) + Schedule (30 pts) + Stability (20 pts) + Trend (20 pts)**

  1. **Progress Component** (0-30 points):
     - Linear mapping: `(completion_percentage / 100) √ó 30`
     - Example: 50% complete ‚Üí 15 points, 80% complete ‚Üí 24 points

  2. **Schedule Component** (0-30 points):
     - Sigmoid function for smooth transitions: `(tanh(buffer_days / 20) + 1) √ó 15`
     - Buffer = days_to_deadline - days_to_completion
     - Examples:
       - 30 days ahead ‚Üí ~28.6 points
       - On schedule (0 buffer) ‚Üí 15 points (neutral)
       - 30 days behind ‚Üí ~1.4 points
     - Returns 15 (neutral) if deadline is missing

  3. **Stability Component** (0-20 points):
     - Velocity consistency via Coefficient of Variation (CV = std_dev / mean)
     - Linear decay: `20 √ó max(0, 1 - (CV / 1.5))`
     - Uses last 10 weeks of completed items (or all available data)
     - Examples:
       - CV = 0 (perfect consistency) ‚Üí 20 points
       - CV = 0.75 (typical) ‚Üí 10 points
       - CV ‚â• 1.5 (chaotic) ‚Üí 0 points
     - Returns 10 (neutral) if insufficient data (<2 weeks) or zero velocity

  4. **Trend Component** (0-20 points):
     - Velocity change between older half vs recent half of data
     - Linear scaling: `clamp(10 + (velocity_change_% / 50) √ó 10, 0, 20)`
     - Examples:
       - +50% velocity growth ‚Üí 20 points
       - 0% change (stable) ‚Üí 10 points (neutral)
       - -50% velocity decline ‚Üí 0 points
     - Returns 10 (neutral) if insufficient trend data (<4 weeks) or zero older velocity

**Key Features**:
- **Smooth gradients**: No threshold-based penalties - incremental changes produce proportional score adjustments
- **Incomplete week filtering**: Automatically excludes current incomplete week (unless today is Sunday ‚â•23:59:59) to prevent mid-week score fluctuations
- **Full 0-100 range**: Formula validated to span entire range from critical projects (<10) to excellent projects (>90)
- **Scales to project size**: Works for 4-week sprints through multi-year projects

**Display**:
- **Primary Value**: 0-100 score displayed prominently (3.5rem font size)
- **Performance Badge**:
  - **Excellent** (80-100): Green badge - project on track
  - **Good** (60-79): Blue badge - minor concerns
  - **Fair** (40-59): Yellow badge - needs attention
  - **At Risk** (0-39): Red badge - significant issues
- **Progress Bar**: Shows completion percentage below health score

**How to Read It**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Project Health Score              ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ         85  /100                    ‚îÇ
‚îÇ       [Excellent]üü¢                 ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë 75.0% Complete
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**What it tells you**:
- **85/100 (Excellent)**: Project healthy - good progress (22.5/30), ahead of schedule (28/30), consistent velocity (18/20), positive trend (16.5/20)
- **75% Complete**: Three-quarters of work finished
- **Green badge**: High confidence in on-time delivery

**Action Guide by Score**:
- **80-100 (Excellent)**: Maintain current pace, celebrate wins, monitor for scope creep
- **60-79 (Good)**: Watch for velocity drops, review upcoming work complexity, address any schedule slippage
- **40-59 (Fair)**: Investigate velocity issues, consider scope reduction, add resources, reassess timeline
- **0-39 (At Risk)**: Emergency intervention needed - scope freeze, add capacity, extend deadline

**Code Location**: `ui/dashboard_cards.py::_calculate_health_score()` and helper functions

---

### 2. Completion Forecast

**What it measures**: Estimated days until project completion

**Calculation Method**:
- **Based on current velocity and remaining work**:
  1. Calculate recent velocity (10-week rolling average or all available data)
  2. Estimate weeks remaining: `remaining_items / current_velocity_items`
  3. Apply PERT factor (default 1.5x) for buffer: `weeks_remaining √ó pert_factor`
  4. Convert to days: `weeks_remaining √ó 7 days`
  5. Add to last data point date to get forecast completion date

**PERT Factor**:
- **Default: 1.5x** (adds 50% buffer for uncertainty)
- **Configurable**: Adjust in Settings based on team's historical accuracy
- **Lower (1.2x)**: High confidence in estimates, predictable velocity
- **Higher (2.0x)**: Low confidence, volatile velocity, complex unknowns

**Confidence Calculation**:
- **Based on velocity consistency (coefficient of variation)**:
  - Low variability (œÉ/Œº < 0.3) ‚Üí High confidence (70-100%)
  - Medium variability (0.3-0.6) ‚Üí Medium confidence (40-69%)
  - High variability (>0.6) ‚Üí Low confidence (<40%)
- Requires at least 3 recent data points

**Display**:
- **Primary Value**: Days remaining (e.g., "45 days remaining")
- **Performance Tier**:
  - **On Track** (80%+ complete): Green
  - **In Progress** (50-79% complete): Blue
  - **Early Stage** (20-49% complete): Yellow
  - **Starting** (<20% complete): Orange
- **Secondary Info**: Completion confidence percentage

**Example**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Completion Forecast    [On Track]üü¢ ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ         45 days remaining           ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  Confidence: 78.5%                  ‚îÇ
‚îÇ  Forecast Date: 2025-12-28          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**What it tells you**:
- **45 days**: Team needs 6-7 weeks at current pace
- **78.5% confidence**: Velocity is fairly consistent (reliable estimate)
- **On Track badge**: Project is progressing well

**Action Guide**:
- **High confidence (>70%)**: Trust the forecast for planning
- **Medium confidence (40-70%)**: Add buffer time for commitments
- **Low confidence (<40%)**: Investigate velocity variability - blockers? scope changes?

**Code Location**: `data/processing.py::calculate_dashboard_metrics()`

---

### 3. Current Velocity

**What it measures**: Team throughput in items and story points per week

**Calculation Method**:
- **Rolling average over recent period**:
  1. Select recent data (10-week window or all available data)
  2. Calculate total items completed in that period
  3. Calculate weeks spanned: `(max_date - min_date) / 7 days`
  4. Velocity = `total_items / weeks_spanned`
- **Separate calculations** for items and story points

**Velocity Trend**:
- **Compares recent velocity vs. historical velocity**:
  - Split data into two halves (older vs. recent)
  - Calculate velocity for each half
  - Compare: `(recent_velocity - older_velocity) / older_velocity`
  - **>10% increase** ‚Üí "Increasing" (accelerating)
  - **¬±10% range** ‚Üí "Stable" (consistent)
  - **>10% decrease** ‚Üí "Decreasing" (slowing)

**Display**:
- **Primary Value**: Items per week (e.g., "8.5 items/week")
- **Performance Tier**:
  - **Accelerating** (increasing trend): Green - team getting faster
  - **Steady** (stable trend): Blue - predictable pace
  - **Slowing** (decreasing trend): Yellow - investigate blockers
  - **Unknown** (insufficient data): Orange
- **Secondary Info**: Story points per week

**Example**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Current Velocity    [Accelerating]üü¢‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ      8.5 items/week                 ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  42.5 points/week                   ‚îÇ
‚îÇ  Trend: Increasing ‚Üë                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**What it tells you**:
- **8.5 items/week**: Team completes ~8-9 items every week
- **42.5 points/week**: Team delivers ~43 story points weekly
- **Accelerating badge**: Velocity improving over time (positive trend)

**Relationship to Flow Velocity**:
- **Project Dashboard Velocity**: Focuses on forecasting project completion
- **Flow Velocity**: Focuses on process health and work type distribution
- Both measure throughput, but serve different purposes

**Action Guide**:
- **Accelerating**: Identify what's working, replicate success patterns
- **Steady**: Maintain current processes, monitor for changes
- **Slowing**: Investigate blockers, technical debt, team capacity issues

**Code Location**: `data/processing.py::calculate_dashboard_metrics()`

---

### 4. Remaining Work

**What it measures**: Work left to complete (items and story points)

**Calculation Method**:
- **Simple subtraction**:
  1. Total scope: `estimated_total_items` and `estimated_total_points` from settings
  2. Completed work: `SUM(completed_items)` and `SUM(completed_points)` from statistics
  3. Remaining work: `total_scope - completed_work`
- **Clamped to zero**: Negative values (overdelivery) display as 0

**Display**:
- **Primary Value**: Remaining items (e.g., "25 items")
- **Performance Tier** (based on completion percentage):
  - **Nearly Complete** (75%+ complete): Green
  - **Halfway** (50-74% complete): Blue
  - **In Progress** (25-49% complete): Yellow
  - **Starting Out** (<25% complete): Orange
- **Secondary Info**: Remaining story points

**Example**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Remaining Work     [Nearly Complete]üü¢‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ         25 items                    ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  125.0 points remaining             ‚îÇ
‚îÇ  75.0% complete                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**What it tells you**:
- **25 items**: One-quarter of work remains
- **125 points**: ~3 weeks of work at 42.5 points/week velocity
- **Nearly Complete badge**: Final stretch - focus on finishing

**Scope Changes**:
- If scope increases (new work added), remaining work can go UP
- Watch for scope creep: Monitor weekly changes in total items/points

**Action Guide**:
- **Nearly Complete (75%+)**: Focus on quality, testing, deployment prep
- **Halfway (50-74%)**: Maintain pace, watch for scope changes
- **In Progress (25-49%)**: Review priorities, consider mid-project adjustments
- **Starting Out (<25%)**: Validate estimates, establish baseline velocity

**Code Location**: `data/processing.py::calculate_dashboard_metrics()`

---

### 5. PERT Timeline

**What it measures**: Optimistic, Most Likely, and Pessimistic completion dates

**PERT (Program Evaluation and Review Technique)**:
- **Project management method** for estimating completion dates under uncertainty
- **Three scenarios**:
  1. **Optimistic**: Best-case scenario (everything goes perfectly)
  2. **Most Likely**: Realistic baseline (typical conditions)
  3. **Pessimistic**: Worst-case scenario (major blockers occur)
- **Weighted average**: PERT estimate = `(Optimistic + 4 √ó Most Likely + Pessimistic) / 6`

**Calculation Method**:
- **Start with base velocity forecast**:
  1. Calculate weeks remaining: `remaining_items / current_velocity_items`
  
- **Apply PERT scenarios**:
  - **Optimistic**: `base_weeks / pert_factor` (divide by 1.5 = 0.67x faster)
  - **Most Likely**: `base_weeks` (baseline scenario)
  - **Pessimistic**: `base_weeks √ó pert_factor` (multiply by 1.5 = 1.5x slower)
  
- **PERT Weighted Average**: `(O + 4M + P) / 6`

**Example Calculation**:
```
Remaining: 30 items
Velocity: 10 items/week
PERT Factor: 1.5

Base weeks: 30 / 10 = 3 weeks

Optimistic: 3 / 1.5 = 2 weeks = 14 days
Most Likely: 3 weeks = 21 days
Pessimistic: 3 √ó 1.5 = 4.5 weeks = 32 days

PERT Estimate: (14 + 4√ó21 + 32) / 6 = 130 / 6 ‚âà 22 days
```

**Display**:
- **Primary Value**: PERT estimate days (e.g., "22 days")
- **Performance Tier**: Based on confidence range width
  - **High Confidence** (range < 14 days): Green
  - **Medium Confidence** (range 14-28 days): Blue
  - **Low Confidence** (range > 28 days): Yellow
- **Secondary Info**: 
  - Optimistic date
  - Most likely date
  - Pessimistic date
  - Confidence range (pessimistic - optimistic)

**Example**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PERT Timeline    [High Confidence]üü¢‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ     PERT Estimate: 22 days          ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚ö° Optimistic: 14 days (Dec 26)   ‚îÇ
‚îÇ  üìä Most Likely: 21 days (Jan 2)   ‚îÇ
‚îÇ  ‚è±Ô∏è  Pessimistic: 32 days (Jan 13)  ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  Range: 18 days                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**What it tells you**:
- **PERT Estimate (22 days)**: Weighted forecast accounting for uncertainty
- **Optimistic (14 days)**: Best-case if no blockers occur
- **Pessimistic (32 days)**: Worst-case if major issues arise
- **18-day range**: Moderate uncertainty (yellow/orange warning)

**When to Use Each Date**:
- **Optimistic**: Internal goal for best-case scenario (motivational)
- **PERT Estimate**: Primary forecast for stakeholder communication
- **Most Likely**: Baseline plan for resource allocation
- **Pessimistic**: Risk planning and buffer time for commitments

**Action Guide by Confidence Range**:
- **<14 days (High Confidence)**: Tight range, predictable - commit to PERT estimate
- **14-28 days (Medium)**: Moderate uncertainty - add 20% buffer to commitments
- **>28 days (Low Confidence)**: High uncertainty - investigate velocity variability, consider re-estimation

**Code Location**: `data/processing.py::calculate_pert_timeline()`

---

### Dashboard Insights (Contextual Guidance)

The Dashboard automatically displays **Key Insights** when actionable conditions are detected. Insights appear below the overview cards with icon, message, and recommended actions.

**Insight Types**:

#### 1. Schedule Variance Insight
**When shown**: Both forecast date and target deadline are available

**Conditions**:
- **Ahead of schedule**: Forecast date ‚â•30 days before deadline
  - Icon: üéØ (target)
  - Color: Green (success)
  - Message: "X days ahead of schedule"
  - Action: "Consider adding scope or advancing deadline"

- **Behind schedule**: Forecast date after deadline
  - Icon: ‚ö†Ô∏è (warning)
  - Color: Red (danger)
  - Message: "X days behind schedule"
  - Action: "Review scope, add resources, or extend deadline"

- **On track**: Forecast within ¬±30 days of deadline
  - Icon: ‚úì (check)
  - Color: Blue (info)
  - Message: "On track for deadline"

**Example**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üí° Key Insights                     ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  üéØ Project is 31 days ahead of     ‚îÇ
‚îÇ     schedule                        ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ     Consider adding scope or        ‚îÇ
‚îÇ     advancing deadline              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 2. Velocity Trend Insight
**When shown**: Velocity trend is detected (increasing/decreasing)

**Conditions**:
- **Increasing velocity**: Team accelerating
  - Icon: ‚Üë (up arrow)
  - Color: Green (success)
  - Message: "Velocity increasing - team accelerating"

- **Decreasing velocity**: Team slowing down
  - Icon: ‚Üì (down arrow)
  - Color: Yellow (warning)
  - Message: "Velocity decreasing - investigate blockers"

**Example**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üí° Key Insights                     ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚Üë Velocity increasing - team       ‚îÇ
‚îÇ    accelerating                     ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚Üì Velocity decreasing -            ‚îÇ
‚îÇ    investigate blockers             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 3. Progress Milestone Insight
**When shown**: Project crosses significant completion thresholds

**Conditions**:
- **50% complete**: Halfway milestone
  - Icon: üèÅ (flag)
  - Color: Blue (info)
  - Message: "Halfway complete! Review lessons learned"

- **75% complete**: Final stretch
  - Icon: üéØ (target)
  - Color: Green (success)
  - Message: "75% complete - entering final stretch"

- **90% complete**: Nearly done
  - Icon: ‚úì (check)
  - Color: Green (success)
  - Message: "90% complete - focus on quality and closure"

**Code Location**: `ui/dashboard_cards.py::create_dashboard_overview_content()`

---

### Practical Dashboard Usage

#### Daily Team Check-in
**What to look at**:
1. **Health Score**: Quick status check - Green? Good! Red? Dig deeper.
2. **Velocity Trend**: Team getting faster or slower?
3. **Insights**: Any actionable alerts?

**Time needed**: 30 seconds

#### Weekly Planning
**What to review**:
1. **Remaining Work**: How much is left?
2. **Current Velocity**: What's our throughput?
3. **Completion Forecast**: Are we on track?
4. **PERT Timeline**: What's the confidence range?

**Actions**:
- Adjust sprint commitments based on velocity
- Flag scope creep if remaining work increased
- Update stakeholder forecasts with PERT estimate

**Time needed**: 5 minutes

#### Stakeholder Updates
**What to communicate**:
1. **Health Score**: Executive summary (one number)
2. **Completion Percentage**: Visual progress indicator
3. **PERT Estimate**: When will it be done?
4. **Schedule Status**: Are we ahead/behind/on-track?

**Presentation tip**: Use the Dashboard tab directly in live demos - single-page view.

**Time needed**: 2 minutes

#### Risk Review (Monthly/Quarterly)
**What to analyze**:
1. **Health Score Trend**: Improving or declining?
2. **Velocity Stability**: Consistent or erratic?
3. **PERT Confidence Range**: Is uncertainty increasing?
4. **Schedule Variance**: Slipping or stable?

**Actions**:
- If health declining: Root cause analysis, corrective action plan
- If velocity erratic: Investigate process issues, team capacity
- If confidence range widening: Re-estimate work, break down unknowns

**Time needed**: 15-30 minutes

---

### Common Dashboard Questions

**Q: Why does remaining work increase sometimes?**  
A: Scope changes - new work added to project. Monitor total items/points in settings.

**Q: Health score dropped but we're completing work - why?**  
A: Velocity trend or schedule health declined. Check insights for specific issues.

**Q: PERT timeline shows huge range - is that normal?**  
A: Large range (>28 days) indicates high uncertainty. Review velocity consistency and re-estimate complex work.

**Q: Velocity is stable but forecast date moved - why?**  
A: Scope changed (remaining work increased) or PERT factor adjusted.

**Q: Can I change the PERT factor?**  
A: Yes, in Settings. Lower (1.2x) for predictable teams, higher (2.0x) for volatile teams.

**Q: What if completion forecast shows "N/A"?**  
A: Either:
- No velocity data (haven't completed any work yet)
- No remaining work (project already complete)
- Velocity is zero (no progress in recent weeks)

**Q: Should Dashboard metrics match Flow metrics exactly?**  
A: No. Dashboard uses 10-week rolling average; Flow uses per-week calculations. They serve different purposes.

---

## Summary Table: Time Units & Aggregations

| Metric                        | Time Unit        | Per-Week Aggregation         | Card Display                               | Secondary Info          | Trend Direction | Chart Y-Axis   |
| ----------------------------- | ---------------- | ---------------------------- | ------------------------------------------ | ----------------------- | --------------- | -------------- |
| **DORA Metrics**              |                  |                              |                                            |                         |                 |                |
| **Deployment Frequency**      | N/A (count)      | SUM (absolute count)         | AVERAGE of weekly counts (16w median avg)  | Releases/week           | ‚Üë green ‚Üì red   | Absolute count |
| **Lead Time**                 | Hours ‚Üí Days     | MEDIAN of all lead times     | AVERAGE of weekly medians (16w median avg) | P95 + Mean              | ‚Üì green ‚Üë red   | Median days    |
| **Change Failure Rate**       | N/A (percentage) | % of failures                | AGGREGATE % across all weeks (16w agg)     | Releases/week           | ‚Üì green ‚Üë red   | Weekly %       |
| **MTTR**                      | Hours            | MEDIAN of all recovery times | AVERAGE of weekly medians (16w median avg) | P95 + Mean              | ‚Üì green ‚Üë red   | Median hours   |
| **Flow Metrics**              |                  |                              |                                            |                         |                 |                |
| **Flow Velocity**             | N/A (count)      | SUM (absolute count)         | Current week count                         | Work type counts        | ‚Üë green ‚Üì red   | Absolute count |
| **Flow Time**                 | Hours ‚Üí Days     | MEDIAN of all flow times     | AVERAGE of weekly medians                  | None                    | ‚Üì green ‚Üë red   | Median days    |
| **Flow Efficiency**           | N/A (percentage) | % active/WIP                 | Current week %                             | None                    | ‚Üë green ‚Üì red   | Weekly %       |
| **Flow Load (WIP)**           | N/A (count)      | COUNT at week end            | Current week count                         | Health badge            | ‚Üì green ‚Üë red   | Absolute count |
| **Flow Distribution**         | N/A (percentage) | % by type                    | Stacked area chart                         | Range indicators        | N/A             | Count by type  |
| **Project Dashboard Metrics** |                  |                              |                                            |                         |                 |                |
| **Project Health Score**      | N/A (0-100)      | N/A (real-time)              | Composite score                            | Progress bar            | ‚Üë green ‚Üì red   | N/A            |
| **Completion Forecast**       | Days             | N/A (real-time)              | Days remaining                             | Confidence %            | ‚Üì green ‚Üë red   | N/A            |
| **Current Velocity**          | Items/week       | 10-week rolling avg          | Items and points per week                  | Trend (‚Üë/‚Üí/‚Üì)           | ‚Üë green ‚Üì red   | N/A            |
| **Remaining Work**            | Items/Points     | N/A (real-time)              | Items and points remaining                 | Completion %            | ‚Üì green ‚Üë red   | N/A            |
| **PERT Timeline**             | Days             | N/A (real-time)              | Optimistic/Most Likely/Pessimistic dates   | Confidence range (days) | N/A             | N/A            |

**Notes**:
- **16w median avg**: Primary values average the weekly medians over a 16-week rolling window
- **16w agg**: CFR aggregates failures/deployments across all 16 weeks (not averaged)
- **P95 + Mean**: Secondary statistics shown below primary value for Lead Time and MTTR
- **Releases/week**: Secondary metric for Deployment Frequency and CFR cards
- **Trend Direction**: Shows which arrow color indicates improvement (green) vs. regression (red)
- **‚Üí (Right Arrow)**: Always gray, indicates exactly 0.0% change (stable)
- **‚àí (Minus)**: Always gray, indicates <5% change or no historical data
- **Real-time (Dashboard)**: Recalculated on every data refresh, not stored as weekly snapshots
- **10-week rolling avg (Velocity)**: Uses last 10 weeks or all available data, whichever is fewer
- **N/A Chart**: Dashboard metrics don't have weekly trend charts (use DORA/Flow tabs for historical analysis)

---

## Integration with Other App Statistics

### Weekly Statistics (`project_data.json`)
- **Burndown Metrics**: Uses same ISO week bucketing
- **Velocity Tracking**: Completed items and story points per week
- **Scope Changes**: Weekly additions to backlog
- **Consistency**: All weekly metrics share same Monday-Sunday boundaries

### Data Points Slider
- **Purpose**: Control historical depth (how many weeks to display)
- **Default**: 12 weeks
- **Applies To**: Both DORA and Flow metrics
- **Performance**: Reading cached snapshots is instant regardless of N

---

## Fallback Behavior & Logging

### When Data is Missing

**Deployment Frequency**:
- No operational tasks ‚Üí 0 deployments
- Future releaseDate ‚Üí Excluded (logged)
- No releaseDate ‚Üí Excluded (logged)

**Lead Time**:
- No "In Deployment" status ‚Üí Falls back to "Done" status
- No operational task ‚Üí Falls back to issue's resolutiondate
- No deployment date ‚Üí Excluded (logged as "no_operational_task_count")

**Change Failure Rate**:
- No change failure field ‚Üí Uses placeholder, all treated as success
- Non-"Yes" values ‚Üí Treated as success

**MTTR**:
- Not "PROD" environment ‚Üí Excluded (logged as "not_production_count")
- No fix deployed date ‚Üí Excluded (logged as "no_fix_deployed_count")
- Case mismatch ("prod" vs "PROD") ‚Üí Included but warning logged

**Flow Metrics**:
- No changelog ‚Üí Issue excluded from Flow Time and Flow Efficiency
- Never entered WIP ‚Üí Excluded from Flow Time (logged as "no_wip_transition_count")

---

## Logging Strategy

### Current Issues
- **Volume**: Verbose logging during calculation (INFO level for every issue)
- **Duration**: 2-minute calculation generates 10,000+ log lines
- **Difficulty**: Hard to track fallbacks and excluded issues

### Recommended Optimization
1. **Reduce Per-Issue Logging**: Use DEBUG level for individual issues
2. **Summary Logging**: INFO level for week-level summaries
3. **Structured Exclusion Counts**: Log counts at end of each week
4. **Progress Indicators**: Show "Week X/N" instead of per-issue progress

Example optimized log:
```
INFO: Week 2025-45: Deployment Frequency: 12 deployments (excluded 2: 1 future, 1 no releaseDate)
INFO: Week 2025-45: Lead Time: Median 48.0h from 25 issues (excluded 8: 3 no deployment, 5 no changelog)
INFO: Week 2025-45: CFR: 8.3% (1/12 failed) (excluded 0)
INFO: Week 2025-45: MTTR: Median 24.0h from 3 bugs (excluded 5: 2 not PROD, 3 no fix)
```

---

## Verification Checklist

To verify metrics are correct:

1. **Check Field Mappings** (`app_settings.json`):
   - `deployment_date`: Field containing deployment/release date (e.g., "fixVersions")
   - `change_failure`: Custom field indicating deployment success/failure
   - `affected_environment`: Field identifying production vs. non-production bugs
   - `effort_category`: Field for work classification (Feature, Tech Debt, Risk)

2. **Check Project Config**:
   - `devops_projects`: List of operational/DevOps project keys
   - `development_projects`: List of development project keys
   - `production_environment_values`: Values that identify production environment (e.g., ["PROD", "Production"])

3. **Check Status Config**:
   - `flow_end_statuses`: Statuses that indicate work is complete
   - `wip_statuses`: Statuses that indicate work is in progress
   - `active_statuses`: Subset of WIP statuses where work is actively being worked on

4. **Check Flow Type Mappings**:
   - Verify `flow_type_mappings` correctly classifies work types
   - Test with sample issues to ensure classification works as expected

5. **Verify Calculations**:
   - Pick one week and manually verify calculations
   - Compare calculated values with expected results
   - Check excluded counts in logs for unexpected filtering

---
- `wip_statuses`: ["Selected", "In Progress", "Testing", ...]
- `active_statuses`: ["In Progress", "In Review", "Testing"]

4. **Check Flow Type Mappings**:
   - Verify `flow_type_mappings` has correct effort category values
   - Test with sample issues to ensure classification works

5. **Verify Calculations**:
   - Pick one week
   - Manually count deployments from operational tasks
   - Compare with metric value
   - Check excluded counts in logs

---

## Performance Optimization Opportunities

### Current Performance
- **Calculation**: ~2 minutes for 12 weeks
- **Display**: Instant (reads from cache)
- **Bottleneck**: Changelog processing for Flow Time/Efficiency

### Optimization Ideas
1. **Parallel Processing**: Calculate weeks in parallel (ThreadPoolExecutor)
2. **Changelog Caching**: Pre-process changelog once, reuse for all metrics
3. **Incremental Updates**: Only recalculate changed weeks
4. **Reduced Logging**: Switch to DEBUG for per-issue logs
5. **Batch JIRA Queries**: Fetch all needed data in one query

Estimated improvement: 2 minutes ‚Üí 30 seconds

---

## Questions & Clarifications

**Q: Why median instead of mean for Lead Time/MTTR?**  
A: Median is robust to outliers. One issue taking 200 hours shouldn't skew the "typical" lead time.

**Q: Why aggregate CFR instead of average?**  
A: Weeks with few deployments would skew the average. Aggregate is more accurate.

**Q: Why is Flow Velocity showing current week, not average?**  
A: Flow Velocity is a "current state" metric (like WIP). For trends, use the scatter chart.

**Q: How do I know if field mapping is wrong?**  
A: Check "excluded_count" in logs. High exclusions = likely mapping issue.

**Q: Can I change the week boundaries?**  
A: Yes, but requires code change in `data/time_period_calculator.py`. ISO weeks are standard.

**Q: Why are DORA metrics showing "N/A"?**  
A: No data found for that metric in the time period. Check:

- Operational tasks exist with fixVersions
- releaseDate is populated and in the past
- Field mappings are correct
- Issues match completion statuses

---

## Practical Examples: Reading the UI

### Example 1: Healthy Team Performance

**Deployment Frequency Card**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Deployment Frequency      [Elite]üü¢ ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ         12.5 deployments/week       ‚îÇ
‚îÇ         (16w median avg)            ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚Üë 8.3% vs prev avg        [green] ‚îÇ
‚îÇ  üì¶ 8.2 releases/week              ‚îÇ
‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ [sparkline] ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
**What it tells you**:
- Elite tier = deploying ‚â•30/month (on track!)
- ‚Üë 8.3% (green) = deployment frequency increasing (good!)
- 12.5 deployments for 8.2 releases = 1.5x deployment-to-release ratio
- Ratio >1 indicates: staging deployments, rollbacks, or multi-environment tracking

**Lead Time Card**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Lead Time for Changes      [High]üîµ ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ            3.2 days                 ‚îÇ
‚îÇ         (16w median avg)            ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚Üì 12.1% vs prev avg      [green]  ‚îÇ
‚îÇ  üìä P95: 5.8d ‚Ä¢ Avg: 4.1d          ‚îÇ
‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ [sparkline] ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
**What it tells you**:
- High tier = 1d-1w lead time (solid performance)
- ‚Üì 12.1% (green) = getting faster (great trend!)
- P95 at 5.8d = 95% of changes deploy within a week
- Gap between median (3.2d) and mean (4.1d) = some outliers pulling average up

**Flow Load Card**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Flow Load (WIP)       [Healthy]üü¢   ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ            8 items                  ‚îÇ
‚îÇ         (current week)              ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚Üì 5.9% vs prev avg       [green]  ‚îÇ
‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ [sparkline] ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
**What it tells you**:
- Healthy badge (<10 items) = team not overloaded
- ‚Üì 5.9% (green) = WIP decreasing (less multitasking)
- Low WIP typically correlates with faster flow time

---

### Example 2: Warning Signs

**Change Failure Rate Card**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Change Failure Rate      [Medium]üü° ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ            18.5%                    ‚îÇ
‚îÇ            (16w agg)                ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚Üë 22.7% vs prev avg       [red]   ‚îÇ
‚îÇ  üì¶ 7.8 releases/week              ‚îÇ
‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ [sparkline] ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
**What it tells you**:
- Medium tier (16-30%) = room for improvement
- ‚Üë 22.7% (red) = failure rate increasing (concerning!)
- Action needed: Review testing processes, deployment procedures
- Investigate: Are failures clustered around specific releases?

**Flow Load Card**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Flow Load (WIP)        [Critical]üî¥ ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ            32 items                 ‚îÇ
‚îÇ         (current week)              ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚Üë 18.2% vs prev avg       [red]   ‚îÇ
‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ [sparkline] ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
**What it tells you**:
- Critical badge (‚â•30 items) = team overloaded
- ‚Üë 18.2% (red) = WIP growing rapidly (danger!)
- High WIP = context switching, slower delivery, burnout risk
- Action needed: Stop starting, start finishing! Implement WIP limits

---

### Example 3: Stable Performance

**MTTR Card**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Mean Time to Recovery    [Elite]üü¢  ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ           0.8 hours                 ‚îÇ
‚îÇ         (16w median avg)            ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚Üí 0.0% vs prev avg       [gray]   ‚îÇ
‚îÇ  üìä P95: 1.2h ‚Ä¢ Avg: 0.9h          ‚îÇ
‚îÇ  ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ [sparkline] ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
**What it tells you**:
- Elite tier (<1h) = bugs fixed quickly (excellent!)
- ‚Üí 0.0% (gray right arrow) = perfectly stable, no change
- Consistent performance = predictable, reliable processes
- P95 at 1.2h = even worst-case bugs fixed in ~1 hour

---

### Example 4: Insufficient Historical Data

**Lead Time Card** (New Project):
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Lead Time for Changes    [Medium]üü° ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ            5.2 days                 ‚îÇ
‚îÇ            (3w data)                ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚àí No trend data yet      [gray]   ‚îÇ
‚îÇ  üìä P95: 8.1d ‚Ä¢ Avg: 6.3d          ‚îÇ
‚îÇ  ‚îÅ‚îÅ [short sparkline] ‚îÅ‚îÅ           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
**What it tells you**:
- ‚àí (minus, gray) = not enough history for trend comparison
- Still shows current performance tier and statistics
- Sparkline is short (only 3 weeks of data)
- Wait for more data before analyzing trends

---

### How to Use These Signals for Team Decisions

**Daily Standup**:
- Glance at WIP health badge ‚Üí Red/Orange = focus on finishing over starting
- Check Flow Velocity trend ‚Üí Declining? Discuss blockers

**Sprint Planning**:
- Review Lead Time P95 ‚Üí Set realistic sprint commitments
- Check Flow Distribution ‚Üí Rebalance if needed (e.g., 80% features, 5% tech debt = unhealthy)

**Retrospective**:
- Compare trend indicators across all metrics
- Green arrows = what's working? (double down!)
- Red arrows = what needs attention? (action items!)

**Leadership Reviews**:
- Performance tier badges = quick executive summary
- Detail charts with zones = show progress toward Elite performance
- Trend indicators = demonstrate continuous improvement (or need for support)

---

## ‚ö†Ô∏è Statistical Limitations and Known Issues

**Last Updated**: November 12, 2025  
**Purpose**: Transparent documentation of statistical limitations, calculation issues, and recommended improvements

### 1. Dashboard Velocity Calculation (‚úÖ FIXED - November 12, 2025)

**Issue**: Velocity calculation used calendar date range instead of actual weeks with data  
**Impact**: Sparse data (common in real projects) produced artificially deflated velocity  
**Example**: 2 weeks of data spanning 9-week period ‚Üí 2.2 items/week (WRONG) instead of 10.0 items/week (CORRECT)

**Fix Applied**:
- ‚úÖ New helper function `calculate_velocity_from_dataframe()` counts unique ISO weeks
- ‚úÖ Dashboard metrics now use actual week count instead of date range
- ‚úÖ Comprehensive test coverage (29 tests) validates fix
- ‚úÖ Backward compatible - continuous data behavior unchanged

**Technical Details**:
```python
# OLD (BUGGY): Used calendar span
weeks = (max_date - min_date).days / 7.0  # ‚ùå Wrong for sparse data
velocity = completed_items / weeks

# NEW (CORRECT): Counts actual weeks with data  
df["iso_week"] = df["date"].dt.strftime("%Y-%U")
unique_weeks = df["iso_week"].nunique()  # ‚úÖ Correct
velocity = completed_items / unique_weeks
```

**Status**: ‚úÖ **RESOLVED** - Fix validated with comprehensive test suite

---

### 2. PERT Timeline Misnomer (‚ö†Ô∏è ACTIVE LIMITATION)

**Issue**: "PERT Timeline" feature is NOT true PERT (Program Evaluation and Review Technique)  
**What it actually is**: Single-point estimate with multiplier-based sensitivity analysis

**Real PERT Method** (3-point estimation):
```
Optimistic = Best case estimate
Most Likely = Expected case estimate  
Pessimistic = Worst case estimate
Expected Value = (O + 4M + P) / 6  # Weighted average
```

**Current Implementation** (multiplier-based):
```
Base Estimate = velocity √ó remaining_items
Optimistic = Base / pert_factor  # e.g., Base / 1.2
Pessimistic = Base √ó pert_factor # e.g., Base √ó 1.2
```

**Why This Matters**:
- Current approach: Symmetric uncertainty (¬±20% for factor 1.2)
- Real PERT: Asymmetric uncertainty (accounts for long-tail risks)
- Example: PERT factor 1.5 creates ¬±50% range, but real projects have asymmetric risk profiles

**User Impact**: Pessimistic scenarios may underestimate tail-risk delays (e.g., major blockers, scope changes)

**Recommended Fix**:
1. **Option A (Quick)**: Rename to "Forecast Range" or "Scenario Analysis" with clear disclaimer
2. **Option B (Better)**: Implement true 3-point PERT with separate O/M/P inputs
3. **Option C (Best)**: Monte Carlo simulation based on historical velocity variance

**Workaround**: Use Pessimistic forecast as "minimum buffer" and add extra padding for high-risk projects

**Status**: ‚ö†Ô∏è **KNOWN LIMITATION** - Rename/disclaimer planned, full PERT implementation future enhancement

---

### 3. Dashboard Confidence Calculation (‚ö†Ô∏è HEURISTIC ONLY)

**Issue**: "Confidence" percentage is NOT a statistical confidence interval  
**What it actually is**: Heuristic based on coefficient of variation (CoV)

**Current Implementation**:
```python
velocity_cv = std_dev / mean  # Coefficient of Variation
confidence_percentage = max(0, 100 - (velocity_cv * 100))

# Example:
mean_velocity = 10 items/week, std_dev = 2 items/week
velocity_cv = 2 / 10 = 0.2 (20% CoV)
confidence = 100 - 20 = 80%  # ‚ùå NOT a statistical confidence interval
```

**Real Statistical Confidence Interval** (95% CI):
```python
# For sample mean with normal distribution
margin_of_error = 1.96 √ó (std_dev / sqrt(n))  # n = sample size
CI_95 = [mean - margin_of_error, mean + margin_of_error]

# Proper confidence statement:
"We are 95% confident the true velocity is between 8.5 and 11.5 items/week"
```

**Why This Matters**:
- Current "80% confidence" does NOT mean "80% probability forecast is accurate"
- It means "velocity has 20% variability relative to mean"
- Users may misinterpret as statistical confidence (e.g., "80% chance we'll finish on time")

**User Impact**: Overconfidence in forecasts with high CoV, underconfidence with low CoV

**Recommended Fix**:
1. **Option A (Quick)**: Rename to "Velocity Stability" or "Forecast Reliability" with disclaimer
2. **Option B (Better)**: Calculate proper confidence intervals using t-distribution
3. **Option C (Best)**: Monte Carlo simulation to generate prediction intervals

**Workaround**: Treat "confidence" as relative indicator (80% = stable velocity, 50% = erratic velocity), not probability

**Status**: ‚ö†Ô∏è **KNOWN LIMITATION** - Rename/disclaimer recommended, proper CI calculation future enhancement

---

### 4. Health Score Weights (‚ö†Ô∏è DESIGN HEURISTICS)

**Issue**: Health score uses design heuristics with no empirical validation  
**Current Formula (v2.1)**:
```python
health_score = (
    progress_component(0-30) +      # 30% weight - completion percentage
    schedule_component(0-30) +      # 30% weight - schedule buffer via sigmoid
    stability_component(0-20) +     # 20% weight - velocity CV
    trend_component(0-20)           # 20% weight - velocity % change
)
```

**Why This Matters**:
- Weights (30/30/20/20) are design decisions balancing delivery vs process metrics, not data-driven
- Different teams may value metrics differently (e.g., schedule adherence vs. velocity stability)
- No empirical validation that these weights predict project success

**Example Impact**:
- Team A: 60% progress (18 pts), on-schedule (15 pts), perfect stability (20 pts), strong growth (18 pts) ‚Üí Health: 71 (Good)
- Team B: 90% progress (27 pts), 30 days behind (1 pt), erratic velocity (8 pts), declining trend (5 pts) ‚Üí Health: 41 (Fair)
- Question: Is Team A "healthier"? Formula emphasizes process consistency over raw progress.

**User Impact**: Health score is relative indicator for trend monitoring, not absolute measure of project health

**Recommended Fix**:
1. **Option A (Implemented)**: Documentation clarifies weights are design heuristics with disclaimer
2. **Option B (Future)**: Allow user-configurable weights based on team priorities
3. **Option C (Future)**: Machine learning model trained on historical project outcomes

**Workaround**: Use health score for trend monitoring (is health improving/declining?), not absolute project assessment. Focus on component breakdowns (progress, schedule, stability, trend) for actionable insights.

**Workaround**: Use health score for trend monitoring (is health improving/declining?), not absolute thresholds

**Status**: ‚ö†Ô∏è **KNOWN LIMITATION** - Disclaimer added to documentation, configurable weights future enhancement

---

### 5. Small Sample Size Effects

**Issue**: Many calculations break down with insufficient data  
**Minimum Data Requirements**:
- **Velocity calculation**: Requires ‚â•2 weeks of data (tested edge case)
- **Velocity trend**: Requires ‚â•4 weeks for meaningful comparison
- **Confidence calculation**: Unstable with <4 weeks (high variance)
- **DORA metrics**: Need ‚â•10 deployments for reliable statistics

**User Impact**: Early-stage projects get unreliable metrics until baseline established

**Current Mitigations**:
- ‚úÖ Edge case handling: Empty dataframes, single data points return safe defaults (0 or None)
- ‚úÖ Test coverage: 29 tests validate edge cases (empty data, single point, zero velocity)
- ‚ö†Ô∏è UI warnings: No explicit "insufficient data" warnings shown to users

**Recommended Fix**:
- Add UI badges: "Baseline Building (week 2/4)" for metrics with <4 weeks data
- Hide unreliable metrics until minimum sample size reached
- Display confidence intervals that widen with small sample sizes

**Workaround**: Treat first 4 weeks as "baseline building" - metrics unreliable until then

**Status**: ‚ö†Ô∏è **PARTIAL MITIGATION** - Edge cases handled in code, UI warnings future enhancement

---

### 6. Assumption: Normally Distributed Data

**Issue**: Many calculations assume normal distribution (e.g., confidence intervals, probability calculations)  
**Reality**: Project data often has skewed distributions:
- **Long-tail delays**: Rare blockers cause extreme outliers (not captured by mean/std dev)
- **Bimodal distributions**: Fast tasks (1-2 days) vs. complex tasks (2-3 weeks)
- **Zero-inflation**: Many weeks with zero deployments (deployment frequency)

**User Impact**: Probability calculations (e.g., "80% chance to meet deadline") may be inaccurate

**Current Mitigations**:
- None - all calculations assume normality

**Recommended Fix**:
1. **Option A**: Add statistical tests (Shapiro-Wilk) to detect non-normal distributions
2. **Option B**: Use non-parametric methods (percentiles, IQR) instead of mean/std dev
3. **Option C**: Bootstrap resampling for robust confidence intervals

**Workaround**: Manually inspect velocity histograms; if skewed, interpret forecasts conservatively

**Status**: ‚ö†Ô∏è **KNOWN LIMITATION** - Normality assumption not validated, robust methods future enhancement

---

### Summary of Current Status

| Issue                              | Severity   | Status                 | Fix Available      |
| ---------------------------------- | ---------- | ---------------------- | ------------------ |
| **Dashboard Velocity Calculation** | üî¥ Critical | ‚úÖ FIXED (Nov 12, 2025) | Yes - deployed     |
| **PERT Timeline Misnomer**         | üü° Medium   | ‚ö†Ô∏è Active               | Rename/disclaimer  |
| **Confidence Heuristic**           | üü° Medium   | ‚ö†Ô∏è Active               | Rename/disclaimer  |
| **Health Score Weights**           | üü° Medium   | ‚ö†Ô∏è Documented           | Disclaimer added   |
| **Small Sample Effects**           | üü¢ Low      | ‚ö†Ô∏è Partial              | Edge cases handled |
| **Normality Assumption**           | üü¢ Low      | ‚ö†Ô∏è Known                | Manual inspection  |

**Overall Assessment**: Dashboard metrics are now **statistically sound** after velocity fix. Remaining limitations are **clearly documented heuristics** (PERT, confidence, health score) and **inherent statistical challenges** (small samples, distribution assumptions) common to all forecasting tools.

---

## Documentation Verification Status

**Last Verified**: November 12, 2025  
**Verified Against**: 
- Code implementation in `data/dora_calculator.py` (v2 functions)
- Code implementation in `data/flow_calculator.py` (v2 functions)
- Code implementation in `data/processing.py` (dashboard functions)
- UI implementation in `ui/dashboard_cards.py`

**Verification Results**:

### DORA Metrics (‚úÖ Verified Correct)
- **Deployment Frequency**: Matches implementation - counts operational tasks with releaseDate, calculates releases separately
- **Lead Time for Changes**: Matches implementation - uses changelog for "In Deployment" timestamp, falls back to "Done" status
- **Change Failure Rate**: Matches implementation - only "Yes" (case-insensitive) marks failure, aggregates across weeks
- **MTTR**: Matches implementation - filters by "PROD" environment (case-sensitive), uses median recovery time

**Key Findings**:
- All time units correctly documented (hours/days conversions match code)
- Median vs. Mean vs. Aggregate calculations accurate
- P95 calculations correctly shown as secondary metrics
- Trend direction logic (‚Üë/‚Üì/‚Üí) matches implementation

### Flow Metrics (‚úÖ Verified Correct)
- **Flow Velocity**: Matches implementation - counts completed items per week, classifies by work type
- **Flow Time**: Matches implementation - measures from first WIP status to resolution
- **Flow Efficiency**: Matches implementation - active time / total WIP time percentage
- **Flow Load (WIP)**: Matches implementation - counts issues in WIP statuses with no resolution
- **Flow Distribution**: Matches implementation - percentage breakdown by Feature/Defect/Tech Debt/Risk

**Key Findings**:
- WIP threshold calculation using Little's Law correctly documented
- Historical reconstruction logic for WIP snapshots accurate
- Flow type classification (primary/secondary tier) matches code
- Work distribution recommended ranges align with Flow Framework standards

### Project Dashboard Metrics (‚úÖ Verified Correct)
- **Project Health Score (v2.1)**: Matches implementation - 4-component continuous formula (progress 30pts, schedule 30pts, stability 20pts, trend 20pts) with incomplete week filtering
- **Completion Forecast**: Matches implementation - uses velocity √ó PERT factor, calculates confidence from CoV
- **Current Velocity**: Matches implementation - 10-week rolling average
- **Remaining Work**: Matches implementation - total scope minus completed work
- **PERT Timeline**: Matches implementation - 3 scenarios with weighted average formula

**Key Findings**:
- Health score v2.1 uses smooth continuous functions (tanh sigmoid, linear decay) instead of threshold-based penalties
- Incomplete week filtering prevents mid-week health score fluctuations (15-30 point drops)
- Formula validated to span full 0-100 range (critical <10, excellent >90)
- Minimum data thresholds: 2 weeks for stability, 4 weeks for trend
- PERT factor application (divide for optimistic, multiply for pessimistic) correct
- Confidence calculation based on coefficient of variation accurate
- Velocity trend detection (>10% threshold) matches code
- Insights logic (schedule variance, velocity trends, milestones) verified

### Common Pitfalls Section (‚úÖ Verified Best Practices)
- Gaming metrics examples drawn from real-world antipatterns
- Metric relationships validated against queueing theory (Little's Law)
- Team comparison warnings align with DORA research guidance

### Mobile-First Work Distribution Chart (‚úÖ Verified Implemented)
- Target zones with 15-18% opacity confirmed in visualization code
- Enhanced hover with percentage, count, and target range verified
- Horizontal legend layout for mobile confirmed
- Progressive disclosure pattern validated

**Areas Enhanced in This Update**:
1. ‚úÖ Added comprehensive Project Dashboard section (not previously documented)
2. ‚úÖ Documented Dashboard Insights feature with 3 insight types
3. ‚úÖ Added practical usage scenarios (daily check-in, weekly planning, stakeholder updates)
4. ‚úÖ Included Dashboard FAQ section with common questions
5. ‚úÖ Updated Summary Table to include Dashboard metrics
6. ‚úÖ Verified all calculations against actual code implementations

**Confidence Level**: **High** - All metrics verified against source code, calculations match implementations, edge cases documented.

---

*Document Version: 1.0 | Last Updated: December 2025*
