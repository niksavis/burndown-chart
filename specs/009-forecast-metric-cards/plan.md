# Implementation Plan: Forecast-Based Metric Cards# Implementation Plan: [FEATURE]



**Branch**: `009-forecast-metric-cards` | **Date**: November 10, 2025 | **Spec**: [spec.md](./spec.md)  **Branch**: `[###-feature-name]` | **Date**: [DATE] | **Spec**: [link]

**Input**: Feature specification from `/specs/009-forecast-metric-cards/spec.md`**Input**: Feature specification from `/specs/[###-feature-name]/spec.md`



**Note**: This plan was generated by the `/speckit.plan` workflow and includes complete Phase 0 research, Phase 1 design artifacts, and constitutional validation.**Note**: This template is filled in by the `/speckit.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.



## Summary## Summary



Add 4-week weighted forecast benchmarks to all Flow and DORA metric cards to provide meaningful context at week start and prevent misleading zero values. Forecasts use weighted averaging (40%, 30%, 20%, 10% for weeks W-0 to W-3) with trend indicators (↗ → ↘) showing current performance vs forecast. Flow Load displays as a range (±20%) instead of point estimate. Implementation extends existing metric card components without changing card structure, adds forecast data to weekly snapshots, and maintains mobile responsiveness.[Extract from feature spec: primary requirement + technical approach from research]



## Technical Context## Technical Context



**Language/Version**: Python 3.13  <!--

**Primary Dependencies**: Dash 2.x, Dash Bootstrap Components, Plotly, pytest    ACTION REQUIRED: Replace the content in this section with the technical details

**Storage**: JSON files (`metrics_snapshots.json`, `project_data.json` via `data/persistence.py`)    for the project. The structure here is presented in advisory capacity to guide

**Testing**: pytest with unit tests (during implementation), integration tests (post-implementation)    the iteration process.

**Target Platform**: Web (PWA), mobile-first responsive design (320px+)  -->

**Project Type**: Web application (Dash single-page app)  

**Performance Goals**: Chart rendering < 500ms, user interactions < 100ms response time  **Language/Version**: [e.g., Python 3.11, Swift 5.9, Rust 1.75 or NEEDS CLARIFICATION]  

**Constraints**: Maintain existing card structure (header/body/collapse), mobile responsiveness, backward compatibility with existing snapshots  **Primary Dependencies**: [e.g., FastAPI, UIKit, LLVM or NEEDS CLARIFICATION]  

**Scale/Scope**: 9 metric cards (4 DORA + 5 Flow), 4-week historical window, weekly snapshot persistence**Storage**: [if applicable, e.g., PostgreSQL, CoreData, files or N/A]  

**Testing**: [e.g., pytest, XCTest, cargo test or NEEDS CLARIFICATION]  

## Constitution Check**Target Platform**: [e.g., Linux server, iOS 15+, WASM or NEEDS CLARIFICATION]

**Project Type**: [single/web/mobile - determines source structure]  

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.***Performance Goals**: [domain-specific, e.g., 1000 req/s, 10k lines/sec, 60 fps or NEEDS CLARIFICATION]  

**Constraints**: [domain-specific, e.g., <200ms p95, <100MB memory, offline-capable or NEEDS CLARIFICATION]  

### Principle I: Layered Architecture ✅ PASS**Scale/Scope**: [domain-specific, e.g., 10k users, 1M LOC, 50 screens or NEEDS CLARIFICATION]

- **Requirement**: Business logic MUST reside in `data/` layer, callbacks MUST only delegate

- **Implementation**: Forecast calculation logic in `data/metrics_calculator.py`, trend comparison in `data/` layer, callbacks in `callbacks/dora_flow_metrics.py` delegate to data functions## Constitution Check

- **Verification**: All forecast calculation functions will have unit tests in `tests/unit/data/test_metrics_calculator.py`

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### Principle II: Test Isolation ✅ PASS

- **Requirement**: Tests MUST NOT create files in project root, use `tempfile` with proper cleanup[Gates determined based on constitution file]

- **Implementation**: Unit tests for forecast calculations use in-memory data structures, integration tests (if any) use temporary JSON files via pytest fixtures

- **Verification**: No tests will write to `metrics_snapshots.json` or `project_data.json` in project root## Project Structure



### Principle III: Performance Budgets ✅ PASS### Documentation (this feature)

- **Requirement**: Initial page load < 2s, chart rendering < 500ms, interactions < 100ms

- **Implementation**: Forecast calculations are simple weighted averages (O(1) with 4 data points), no additional API calls, minimal DOM updates (add 2 lines per card)```text

- **Impact**: Negligible performance impact (~5ms per forecast calculation, 9 metrics = ~45ms total)specs/[###-feature]/

├── plan.md              # This file (/speckit.plan command output)

### Principle IV: Simplicity & Reusability ✅ PASS├── research.md          # Phase 0 output (/speckit.plan command)

- **Requirement**: Keep implementations simple (KISS), avoid duplication (DRY)├── data-model.md        # Phase 1 output (/speckit.plan command)

- **Implementation**: Single reusable function `calculate_forecast(historical_values, weights)` for all metrics, trend indicator logic extracted to `calculate_trend_vs_forecast(current, forecast, metric_type)`, Flow Load range calculation is a parameterized variant├── quickstart.md        # Phase 1 output (/speckit.plan command)

- **Verification**: No code duplication across metric types, all calculation logic testable in isolation├── contracts/           # Phase 1 output (/speckit.plan command)

└── tasks.md             # Phase 2 output (/speckit.tasks command - NOT created by /speckit.plan)

### Principle V: Data Privacy & Security ✅ PASS```

- **Requirement**: No customer-identifying information in code/docs/examples

- **Implementation**: Design spec already uses placeholder data ("Example Organization"), forecast examples use generic metric values (12-18 items, ~13 items/week)### Source Code (repository root)

- **Verification**: No JIRA field IDs, company names, or production domains in implementation code<!--

  ACTION REQUIRED: Replace the placeholder tree below with the concrete layout

**GATE STATUS**: ✅ ALL CHECKS PASS - Proceed to Phase 0  for this feature. Delete unused options and expand the chosen structure with

  real paths (e.g., apps/admin, packages/something). The delivered plan must

## Project Structure  not include Option labels.

-->

### Documentation (this feature)

```text

```text# [REMOVE IF UNUSED] Option 1: Single project (DEFAULT)

specs/009-forecast-metric-cards/src/

├── spec.md              # Feature specification (input)├── models/

├── plan.md              # This file (Phase 0-1 planning output)├── services/

├── research.md          # Phase 0: Architecture decisions (COMPLETE)├── cli/

├── data-model.md        # Phase 1: Forecast data structures (COMPLETE)└── lib/

├── quickstart.md        # Phase 1: Developer onboarding guide (COMPLETE)

├── contracts/           # Phase 1: API contracts (COMPLETE)tests/

│   ├── forecast-calculator.md    # calculate_forecast() interface├── contract/

│   ├── trend-indicator.md        # calculate_trend_vs_forecast() interface├── integration/

│   └── flow-load-range.md        # calculate_flow_load_range() interface└── unit/

└── tasks.md             # Phase 2: 99 implementation tasks (COMPLETE)

```# [REMOVE IF UNUSED] Option 2: Web application (when "frontend" + "backend" detected)

backend/

### Source Code (repository root)├── src/

│   ├── models/

```text│   ├── services/

data/│   └── api/

├── metrics_calculator.py      # NEW: Forecast calculation logic└── tests/

│   ├── calculate_forecast()           # 4-week weighted average

│   ├── calculate_trend_vs_forecast()  # Trend direction & percentagefrontend/

│   └── calculate_flow_load_range()    # Range calculation for WIP├── src/

├── metrics_snapshots.py       # MODIFY: Add forecast data to snapshots│   ├── components/

│   └── save_metrics_snapshot()        # Enhanced with forecast field│   ├── pages/

└── persistence.py             # EXISTING: JSON file operations (unchanged)│   └── services/

└── tests/

ui/

├── metric_cards.py            # MODIFY: Add forecast display to cards# [REMOVE IF UNUSED] Option 3: Mobile + API (when "iOS/Android" detected)

│   └── create_metric_card()           # Enhanced with forecast sectionapi/

├── dora_metrics_dashboard.py  # MODIFY: Pass forecast data to cards└── [same as backend above]

└── flow_metrics_dashboard.py  # MODIFY: Pass forecast data to cards

ios/ or android/

callbacks/└── [platform-specific structure: feature modules, UI flows, platform tests]

└── dora_flow_metrics.py       # MODIFY: Calculate and pass forecast data```

    ├── update_dora_metrics()          # Enhanced with forecast calculation

    └── update_flow_metrics()          # Enhanced with forecast calculation**Structure Decision**: [Document the selected structure and reference the real

directories captured above]

tests/

├── unit/## Complexity Tracking

│   └── data/

│       └── test_metrics_calculator.py  # NEW: Forecast calculation tests> **Fill ONLY if Constitution Check has violations that must be justified**

│           ├── test_calculate_forecast()

│           ├── test_calculate_trend_vs_forecast()| Violation | Why Needed | Simpler Alternative Rejected Because |

│           ├── test_calculate_flow_load_range()|-----------|------------|-------------------------------------|

│           ├── test_forecast_with_insufficient_data()| [e.g., 4th project] | [current need] | [why 3 projects insufficient] |

│           └── test_forecast_edge_cases()| [e.g., Repository pattern] | [specific problem] | [why direct DB access insufficient] |

└── integration/
    └── test_metric_cards_with_forecast.py  # NEW: End-to-end card display

configuration/
└── metrics_config.py          # MODIFY: Add forecast config constants
    ├── FORECAST_WEIGHTS = [0.1, 0.2, 0.3, 0.4]
    ├── FORECAST_TREND_THRESHOLD = 0.10  # ±10%
    ├── FLOW_LOAD_RANGE_PERCENT = 0.20   # ±20%
    ├── FORECAST_MIN_WEEKS = 2
    ├── FORECAST_DECIMAL_PRECISION = 1
    ├── HIGHER_BETTER_METRICS list
    └── LOWER_BETTER_METRICS list
```

**Structure Decision**: Web application structure with layered architecture. Forecast feature extends existing DORA/Flow metrics system without introducing new top-level modules. All business logic resides in `data/` layer per Constitution Principle I. UI components in `ui/` layer are enhanced to display forecast data. Callbacks remain thin orchestration layer delegating to data functions.

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

**No violations detected** - All constitutional principles are satisfied by this implementation approach.

---

## Implementation Milestones

**Total Estimated Duration**: 3 days (16 hours total, 7 hours for MVP)

### M1: Core Calculation Logic (Day 1 - 2 hours)
- Implement `data/metrics_calculator.py` with all 3 functions
- Unit tests achieving >95% coverage
- Configuration constants in `configuration/metrics_config.py`

### M2: Data Persistence (Day 1-2 - 2 hours)
- Enhance `data/metrics_snapshots.py` with forecast fields
- Backward compatibility tests with legacy snapshots
- Verify JSON schema correctness

### M3: UI Components (Day 2 - 4 hours)
- Modify `ui/metric_cards.py` to display forecast
- Mobile responsiveness verification (320px, 375px, 768px)
- Visual consistency with existing card design

### M4: Callback Integration (Day 2-3 - 2 hours)
- Update DORA metrics callbacks
- Update Flow metrics callbacks
- End-to-end workflow testing

### M5: Testing & Validation (Day 3 - 4 hours)
- Integration tests for all 9 metric cards
- Performance validation (< 500ms rendering, < 100ms calculation)
- Cross-browser testing (Chrome, Firefox, Safari)
- Mobile device testing

### M6: Documentation & Deployment (Day 3 - 2 hours)
- Update user-facing help content
- Update `.github/copilot-instructions.md`
- Create migration guide for teams with existing data
- Prepare merge to main branch

---

## Post-Phase 1 Constitution Re-Check

**Status**: ✅ ALL PRINCIPLES STILL SATISFIED

### Re-verification After Design

1. **Layered Architecture**: `data/metrics_calculator.py` contains all business logic, callbacks delegate
2. **Test Isolation**: Unit tests use in-memory data, no project root file pollution
3. **Performance Budgets**: Forecast calculation O(1) with 4 values, <50ms total overhead
4. **Simplicity & Reusability**: Single `calculate_forecast()` function reused across 9 metrics
5. **Data Privacy**: No customer data in contracts, examples use placeholder values

**Design Impact**: None - design reinforces constitutional principles through:
- Clear separation of concerns (calculation → storage → display)
- Simple, testable functions with explicit contracts
- Minimal performance overhead
- DRY principle applied (single forecast algorithm for all metrics)

---

## Success Criteria

### Functional
- ✅ All 9 metric cards display forecast benchmarks
- ✅ Trend indicators show current vs forecast comparison
- ✅ Flow Load displays as range (±20%)
- ✅ Historical weeks show achieved vs forecast
- ✅ Edge cases handled (insufficient data, week start, zeros)
- ✅ Badge logic uses W-1 performance on Monday morning

### Non-Functional
- ✅ Unit test coverage > 95% for `data/metrics_calculator.py`
- ✅ No performance regression (< 50ms overhead per dashboard load)
- ✅ Mobile responsive on 320px+ screens
- ✅ Backward compatible with existing snapshots

### Constitutional Compliance
- ✅ All business logic in `data/` layer
- ✅ Zero test pollution (no files in project root)
- ✅ Performance budgets maintained
- ✅ Code reuse via shared forecast function
- ✅ No customer data exposure

---

## Next Steps

**✅ PLANNING COMPLETE** - All artifacts generated, ready for implementation

### Available Artifacts:
1. ✅ `spec.md` - Feature specification with 5 user stories, 18 requirements
2. ✅ `research.md` - 5 architectural decisions documented
3. ✅ `data-model.md` - 3 entities defined with validation rules
4. ✅ `contracts/` - 3 API contracts with function signatures
5. ✅ `quickstart.md` - 6-step developer guide (~2 hours)
6. ✅ `tasks.md` - 99 tasks organized by user stories
7. ✅ `plan.md` - This file (implementation roadmap)

### Implementation Options:

**Option 1: MVP First (Recommended - 7 hours)**
1. Complete Phase 1: Setup (T001-T004)
2. Complete Phase 2: Foundational (T005-T021)
3. Complete Phase 3: User Story 1 (T022-T039a)
4. Deploy and validate with users

**Option 2: Full Feature (16 hours)**
1. All 8 phases from tasks.md
2. Complete feature set with all user stories

**Option 3: Start Implementation Manually**
```powershell
# Follow the quickstart.md guide
# Or start with first task from tasks.md
.\.venv\Scripts\activate
# Create data/metrics_calculator.py and begin implementing
```

**Branch**: `009-forecast-metric-cards`  
**Ready to implement**: Yes - all planning artifacts complete and validated
