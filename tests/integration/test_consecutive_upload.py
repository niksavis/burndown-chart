#!/usr/bin/env python3
"""
Integration test for consecutive JSON upload functionality.

This test validates the fix for the upload bug where consecutive uploads
of the same JSON file would fail after the first upload. The fix involved
adding upload content clearing to the Dash callback to ensure subsequent
uploads of the same file can trigger the callback properly.

Key tests:
- JSON processing works correctly for upload format
- Multiple consecutive uploads of same content succeed
- Callback signature includes upload content clearing
- Upload processing is robust across multiple scenarios

This test can be run via VS Code's Test Explorer or pytest directly.
"""

import json
import base64
import pytest
import sys
from pathlib import Path
from unittest.mock import patch

# Add the project root to the Python path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))


class TestConsecutiveUpload:
    """Test consecutive JSON upload functionality."""

    @pytest.fixture
    def sample_json_data(self):
        """Sample JSON data for testing uploads."""
        return {
            "issues": [
                {
                    "key": "TEST-1",
                    "fields": {
                        "summary": "Test Issue 1",
                        "status": {"name": "Done"},
                        "created": "2024-01-01T10:00:00.000+0000",
                        "resolutiondate": "2024-01-05T10:00:00.000+0000",
                    },
                },
                {
                    "key": "TEST-2",
                    "fields": {
                        "summary": "Test Issue 2",
                        "status": {"name": "In Progress"},
                        "created": "2024-01-02T10:00:00.000+0000",
                        "resolutiondate": None,
                    },
                },
            ]
        }

    @pytest.fixture
    def encoded_json_content(self, sample_json_data):
        """Encode JSON data as base64 for upload simulation."""
        json_string = json.dumps(sample_json_data)
        json_bytes = json_string.encode("utf-8")
        return base64.b64encode(json_bytes).decode("ascii")

    def test_consecutive_json_uploads_work(
        self, sample_json_data, encoded_json_content
    ):
        """Test that consecutive uploads of the same JSON file work correctly."""

        # Simulate the upload processing logic that should happen
        upload_contents = f"data:application/json;base64,{encoded_json_content}"

        # Test the data processing part (what happens inside the callback)
        content_type, content_string = upload_contents.split(",")
        decoded = base64.b64decode(content_string)
        json_data = json.loads(decoded.decode("utf-8"))

        # Verify the JSON was properly decoded
        assert "issues" in json_data
        assert len(json_data["issues"]) == 2
        assert json_data["issues"][0]["key"] == "TEST-1"
        assert json_data["issues"][1]["key"] == "TEST-2"

        # Mock the data persistence to test multiple uploads
        with (
            patch("data.persistence.save_statistics") as mock_save,
            patch("data.processing.read_and_clean_data") as mock_process,
        ):
            mock_process.return_value = (["processed_data"], False)
            mock_save.return_value = None

            # The key test: verify that the same content can be processed multiple times
            # This simulates what the callback should be able to do now

            # First processing
            result1 = self._simulate_upload_processing(upload_contents)
            assert result1 is not None

            # Second processing with same content - this should also work now
            result2 = self._simulate_upload_processing(upload_contents)
            assert result2 is not None

            # Both should succeed (before the fix, second would fail)
            assert result1 == result2

    def _simulate_upload_processing(self, upload_contents):
        """Simulate the upload processing that happens in the callback."""
        try:
            content_type, content_string = upload_contents.split(",")
            decoded = base64.b64decode(content_string)
            json_data = json.loads(decoded.decode("utf-8"))
            return json_data
        except Exception:
            return None

    def test_upload_callback_signature_includes_content_clearing(self):
        """Test that the callback signature includes the upload content clearing output."""

        # Read the callback file to verify structure
        callback_file = (
            Path(__file__).parent.parent.parent / "callbacks" / "statistics.py"
        )
        with open(callback_file, "r") as f:
            content = f.read()

        # Verify the callback has the upload-data contents output for clearing
        assert 'Output("upload-data", "contents", allow_duplicate=True)' in content, (
            "Callback missing upload content clearing output"
        )

        # Verify successful returns clear upload contents
        assert 'return df.to_dict("records"), False, no_update, None' in content, (
            "Callback not clearing upload contents on success"
        )

        # Verify error returns preserve upload contents
        assert "return rows, is_sample_data, no_update, no_update" in content, (
            "Callback not preserving upload contents on error"
        )

    def test_json_processing_functionality(
        self, sample_json_data, encoded_json_content
    ):
        """Test that JSON processing works correctly for the upload format."""

        upload_contents = f"data:application/json;base64,{encoded_json_content}"

        # Test the base64 decoding
        content_type, content_string = upload_contents.split(",")
        decoded = base64.b64decode(content_string)
        json_data = json.loads(decoded.decode("utf-8"))

        # Verify structure
        assert json_data == sample_json_data
        assert len(json_data["issues"]) == 2

        # Verify individual issue data
        issue1 = json_data["issues"][0]
        assert issue1["key"] == "TEST-1"
        assert issue1["fields"]["status"]["name"] == "Done"
        assert issue1["fields"]["resolutiondate"] is not None

        issue2 = json_data["issues"][1]
        assert issue2["key"] == "TEST-2"
        assert issue2["fields"]["status"]["name"] == "In Progress"
        assert issue2["fields"]["resolutiondate"] is None

    def test_multiple_upload_scenarios(self, sample_json_data, encoded_json_content):
        """Test various upload scenarios to ensure robustness."""

        upload_contents = f"data:application/json;base64,{encoded_json_content}"

        # Test multiple consecutive processing of same content
        results = []
        for i in range(5):  # Try 5 consecutive uploads
            result = self._simulate_upload_processing(upload_contents)
            results.append(result)

        # All results should be identical and successful
        assert all(r is not None for r in results), "Some upload processing failed"
        assert all(r == results[0] for r in results), "Upload results are inconsistent"

        # Verify the data structure is preserved across all uploads
        for result in results:
            assert "issues" in result
            assert len(result["issues"]) == 2


if __name__ == "__main__":
    # Allow running this test file directly
    pytest.main([__file__, "-v"])
