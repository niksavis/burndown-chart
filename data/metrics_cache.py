"""Metrics calculation caching with TTL.

This module provides caching functionality for calculated DORA and Flow metrics
to improve performance and reduce redundant calculations.

Cache invalidation triggers:
- TTL expiration (default 1 hour)
- Field mapping hash changes
- Manual refresh
- Cache version mismatch
"""

import json
import logging
import os
from datetime import datetime, timedelta, timezone
from typing import Dict, Optional

logger = logging.getLogger(__name__)

# Cache file configuration
CACHE_FILE = "metrics_cache.json"
CACHE_VERSION = "1.0"
MAX_ENTRIES = 100
DEFAULT_TTL_SECONDS = 3600  # 1 hour

# Eviction policy
EVICTION_POLICY = "LRU"  # Least Recently Used


def generate_cache_key(
    metric_type: str, start_date: str, end_date: str, field_hash: str
) -> str:
    """Generate unique cache key for metrics.

    Args:
        metric_type: "dora" or "flow"
        start_date: ISO 8601 start date
        end_date: ISO 8601 end date
        field_hash: MD5 hash of field mappings

    Returns:
        Cache key string

    Example:
        >>> generate_cache_key("dora", "2025-01-01", "2025-01-31", "a3f5c8d9")
        "dora_2025-01-01_2025-01-31_a3f5c8d9"
    """
    return f"{metric_type}_{start_date}_{end_date}_{field_hash}"


def load_cached_metrics(cache_key: str) -> Optional[Dict]:
    """Load metrics from cache if valid.

    Args:
        cache_key: Cache key generated by generate_cache_key()

    Returns:
        Dictionary of metrics if cache hit and valid, None otherwise

    Cache miss reasons:
    - Cache file doesn't exist
    - Cache key not found
    - Entry expired (past TTL)
    - Cache version mismatch
    """
    if not os.path.exists(CACHE_FILE):
        logger.debug(f"Cache miss: {cache_key} (file not found)")
        return None

    try:
        with open(CACHE_FILE, "r") as f:
            content = f.read().strip()
            if not content:  # Empty file
                logger.debug(f"Cache miss: {cache_key} (empty cache file)")
                return None
            cache = json.loads(content)

        # Check cache version
        if cache.get("cache_version") != CACHE_VERSION:
            logger.warning(
                f"Cache version mismatch: expected {CACHE_VERSION}, "
                f"got {cache.get('cache_version')}"
            )
            return None

        # Check if key exists
        if cache_key not in cache.get("entries", {}):
            logger.debug(f"Cache miss: {cache_key} (key not found)")
            return None

        entry = cache["entries"][cache_key]

        # Check expiration
        expires_at = datetime.fromisoformat(entry["expires_at"])
        if datetime.now(timezone.utc) > expires_at:
            logger.debug(f"Cache miss: {cache_key} (expired)")
            return None

        # Update access time for LRU eviction
        entry["last_accessed"] = datetime.now(timezone.utc).isoformat()
        with open(CACHE_FILE, "w") as f:
            json.dump(cache, f, indent=2)

        logger.info(f"Cache hit: {cache_key}")
        return entry["metrics"]

    except Exception as e:
        logger.error(f"Error loading cache: {e}")
        return None


def save_cached_metrics(
    cache_key: str, metrics: Dict, ttl_seconds: int = DEFAULT_TTL_SECONDS
) -> bool:
    """Save calculated metrics to cache.

    Args:
        cache_key: Cache key generated by generate_cache_key()
        metrics: Dictionary of calculated metrics
        ttl_seconds: Time-to-live in seconds (default: 3600)

    Returns:
        True if save successful, False otherwise
    """
    try:
        # Load existing cache or create new
        cache = None
        if os.path.exists(CACHE_FILE):
            try:
                with open(CACHE_FILE, "r") as f:
                    content = f.read().strip()
                    if content:  # Check if file has content
                        cache = json.loads(content)
            except (json.JSONDecodeError, ValueError):
                logger.warning("Cache file corrupted, recreating")
                cache = None

        if cache is None:
            cache = {
                "cache_version": CACHE_VERSION,
                "entries": {},
                "max_entries": MAX_ENTRIES,
                "eviction_policy": EVICTION_POLICY,
            }

        # Calculate timestamps
        calculated_at = datetime.now(timezone.utc)
        expires_at = calculated_at + timedelta(seconds=ttl_seconds)

        # Create cache entry
        entry = {
            "cache_key": cache_key,
            "metrics": metrics,
            "calculated_at": calculated_at.isoformat(),
            "expires_at": expires_at.isoformat(),
            "last_accessed": calculated_at.isoformat(),
            "ttl_seconds": ttl_seconds,
        }

        # Add entry to cache
        cache["entries"][cache_key] = entry

        # Enforce max entries with LRU eviction
        if len(cache["entries"]) > MAX_ENTRIES:
            _evict_lru_entries(cache)

        # Save to file
        with open(CACHE_FILE, "w") as f:
            json.dump(cache, f, indent=2)

        logger.info(f"Cached metrics: {cache_key} (TTL: {ttl_seconds}s)")
        return True

    except Exception as e:
        logger.error(f"Error saving cache: {e}")
        return False


def _evict_lru_entries(cache: Dict) -> None:
    """Evict least recently used entries to stay under MAX_ENTRIES.

    Args:
        cache: Cache dictionary (modified in-place)
    """
    entries = cache.get("entries", {})

    # Sort by last_accessed timestamp (oldest first)
    sorted_keys = sorted(
        entries.keys(), key=lambda k: entries[k].get("last_accessed", "")
    )

    # Remove oldest entries until under limit
    while len(entries) > MAX_ENTRIES:
        oldest_key = sorted_keys.pop(0)
        del entries[oldest_key]
        logger.debug(f"Evicted cache entry: {oldest_key} (LRU policy)")


def invalidate_cache(cache_key: Optional[str] = None) -> bool:
    """Invalidate cache entries.

    Args:
        cache_key: Specific key to invalidate, or None to clear all

    Returns:
        True if invalidation successful, False otherwise
    """
    try:
        if cache_key is None:
            # Clear entire cache
            if os.path.exists(CACHE_FILE):
                os.remove(CACHE_FILE)
                logger.info("Cleared entire metrics cache")
            return True

        # Remove specific entry
        if not os.path.exists(CACHE_FILE):
            return True  # Nothing to invalidate

        with open(CACHE_FILE, "r") as f:
            content = f.read().strip()
            if not content:  # Empty file
                return True
            cache = json.loads(content)

        if cache_key in cache.get("entries", {}):
            del cache["entries"][cache_key]

            with open(CACHE_FILE, "w") as f:
                json.dump(cache, f, indent=2)

            logger.info(f"Invalidated cache entry: {cache_key}")

        return True

    except Exception as e:
        logger.error(f"Error invalidating cache: {e}")
        return False


def get_cache_stats() -> Dict:
    """Get cache statistics for monitoring.

    Returns:
        Dictionary with cache statistics:
        {
            "total_entries": int,
            "valid_entries": int,
            "expired_entries": int,
            "cache_file_size_kb": float,
            "oldest_entry": str (ISO 8601),
            "newest_entry": str (ISO 8601)
        }
    """
    if not os.path.exists(CACHE_FILE):
        return {
            "total_entries": 0,
            "valid_entries": 0,
            "expired_entries": 0,
            "cache_file_size_kb": 0,
            "oldest_entry": None,
            "newest_entry": None,
        }

    try:
        with open(CACHE_FILE, "r") as f:
            content = f.read().strip()
            if not content:  # Empty file
                return {
                    "total_entries": 0,
                    "valid_entries": 0,
                    "expired_entries": 0,
                    "cache_file_size_kb": 0,
                    "oldest_entry": None,
                    "newest_entry": None,
                }
            cache = json.loads(content)

        entries = cache.get("entries", {})
        now = datetime.now(timezone.utc)

        # Count valid vs expired
        valid_count = 0
        expired_count = 0
        for entry in entries.values():
            expires_at = datetime.fromisoformat(entry["expires_at"])
            if now <= expires_at:
                valid_count += 1
            else:
                expired_count += 1

        # Find oldest and newest
        calculated_times = [
            entry.get("calculated_at")
            for entry in entries.values()
            if entry.get("calculated_at")
        ]
        oldest = min(calculated_times) if calculated_times else None
        newest = max(calculated_times) if calculated_times else None

        # File size
        file_size_bytes = os.path.getsize(CACHE_FILE)
        file_size_kb = file_size_bytes / 1024

        return {
            "total_entries": len(entries),
            "valid_entries": valid_count,
            "expired_entries": expired_count,
            "cache_file_size_kb": round(file_size_kb, 2),
            "oldest_entry": oldest,
            "newest_entry": newest,
        }

    except Exception as e:
        logger.error(f"Error getting cache stats: {e}")
        return {
            "total_entries": 0,
            "valid_entries": 0,
            "expired_entries": 0,
            "cache_file_size_kb": 0,
            "oldest_entry": None,
            "newest_entry": None,
        }
